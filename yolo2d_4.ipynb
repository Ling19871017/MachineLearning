{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import cv2\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.1, dtype=tf.float32):\n",
    "    x = tf.cast(x, dtype=dtype)\n",
    "    bool_mask = (x > 0)\n",
    "    mask = tf.cast(bool_mask, dtype=dtype)\n",
    "    return 1.0 * mask * x + alpha * (1 - mask) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(scope, input, kernel_size, stride=1, pretrain=True, train=True):\n",
    "    \"\"\"convolutional layer\n",
    "\n",
    "    Args:\n",
    "      input: 4-D tensor [batch_size, height, width, depth]\n",
    "      scope: variable_scope name\n",
    "      kernel_size: [k_height, k_width, in_channel, out_channel]\n",
    "      stride: int32\n",
    "    Return:\n",
    "      output: 4-D tensor [batch_size, height/stride, width/stride, out_channels]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "#         kernel = _variable_with_weight_decay('weights', shape = kernel_size, stddev = 5e-2)\n",
    "#         var = self._variable_on_cpu(name, shape, tf.truncated_normal_initializer(stddev=stddev, dtype=tf.float32))\n",
    "        kernel = tf.get_variable('weights', kernel_size, initializer=tf.truncated_normal_initializer(stddev=5e-2, dtype=tf.float32), dtype=tf.float32)\n",
    "        conv = tf.nn.conv2d(input, kernel, [1, stride, stride, 1], padding='SAME')\n",
    "        biases = tf.get_variable('biases', kernel_size[3:], initializer = tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "#         biases = self._variable_on_cpu('biases', kernel_size[3:], tf.constant_initializer(0.0), pretrain, train)\n",
    "        conv_plus_biases = tf.nn.bias_add(conv, biases)\n",
    "        conv = leaky_relu(conv_plus_biases)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(input, kernel_size, stride):\n",
    "    \"\"\"max_pool layer\n",
    "\n",
    "    Args:\n",
    "      input: 4-D tensor [batch_zie, height, width, depth]\n",
    "      kernel_size: [k_height, k_width]\n",
    "      stride: int32\n",
    "    Return:\n",
    "      output: 4-D tensor [batch_size, height/stride, width/stride, depth]\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(input, ksize=[1, kernel_size[0], kernel_size[1], 1], strides=[1, stride, stride, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def local(scope, input, in_dimension, out_dimension, leaky=True):\n",
    "    \"\"\"Fully connection layer\n",
    "\n",
    "    Args:\n",
    "      scope: variable_scope name\n",
    "      input: [batch_size, ???]\n",
    "      out_dimension: int32\n",
    "    Return:\n",
    "      output: 2-D tensor [batch_size, out_dimension]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        reshape = tf.reshape(input, [tf.shape(input)[0], -1])\n",
    "\n",
    "#         weights = self._variable_with_weight_decay('weights', shape=[in_dimension, out_dimension],\n",
    "#                                                  stddev=0.04, wd=self.weight_decay, pretrain=pretrain, train=train)\n",
    "#         var = self._variable_on_cpu(name, shape,\n",
    "#       tf.truncated_normal_initializer(stddev=stddev, dtype=tf.float32), pretrain, train)\n",
    "        weights = tf.get_variable('weights', [in_dimension, out_dimension], initializer=tf.truncated_normal_initializer(stddev=0.04, dtype=tf.float32), dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "#         biases = self._variable_on_cpu('biases', [out_dimension], tf.constant_initializer(0.0), pretrain, train)\n",
    "        biases = tf.get_variable('biases', [out_dimension], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "        \n",
    "        local = tf.matmul(reshape, weights) + biases\n",
    "\n",
    "        if leaky:\n",
    "            local = leaky_relu(local)\n",
    "        else:\n",
    "            local = tf.identity(local, name=scope.name)\n",
    "\n",
    "    return local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cond1(num, object_num, loss, predict, label, nilboy):\n",
    "    \"\"\"\n",
    "    if num < object_num\n",
    "    \"\"\"\n",
    "    return num < object_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iou(boxes1, boxes2):\n",
    "    \"\"\"calculate ious\n",
    "    Args:\n",
    "      boxes1: 4-D tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4]  ====> (x_center, y_center, w, h)\n",
    "      boxes2: 1-D tensor [4] ===> (x_center, y_center, w, h)\n",
    "    Return:\n",
    "      iou: 3-D tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    \"\"\"\n",
    "    boxes1 = tf.stack([boxes1[:, :, :, 0] - boxes1[:, :, :, 2] / 2, boxes1[:, :, :, 1] - boxes1[:, :, :, 3] / 2,\n",
    "                      boxes1[:, :, :, 0] + boxes1[:, :, :, 2] / 2, boxes1[:, :, :, 1] + boxes1[:, :, :, 3] / 2])\n",
    "    boxes1 = tf.transpose(boxes1, [1, 2, 3, 0])\n",
    "    boxes2 =  tf.stack([boxes2[0] - boxes2[2] / 2, boxes2[1] - boxes2[3] / 2,\n",
    "                      boxes2[0] + boxes2[2] / 2, boxes2[1] + boxes2[3] / 2])\n",
    "\n",
    "    #calculate the left up point\n",
    "    lu = tf.maximum(boxes1[:, :, :, 0:2], boxes2[0:2])\n",
    "    rd = tf.minimum(boxes1[:, :, :, 2:], boxes2[2:])\n",
    "\n",
    "    #intersection\n",
    "    intersection = rd - lu \n",
    "\n",
    "    inter_square = intersection[:, :, :, 0] * intersection[:, :, :, 1]\n",
    "\n",
    "    mask = tf.cast(intersection[:, :, :, 0] > 0, tf.float32) * tf.cast(intersection[:, :, :, 1] > 0, tf.float32)\n",
    "    \n",
    "    inter_square = mask * inter_square\n",
    "    \n",
    "    #calculate the boxs1 square and boxs2 square\n",
    "    square1 = (boxes1[:, :, :, 2] - boxes1[:, :, :, 0]) * (boxes1[:, :, :, 3] - boxes1[:, :, :, 1])\n",
    "    square2 = (boxes2[2] - boxes2[0]) * (boxes2[3] - boxes2[1])\n",
    "    \n",
    "    return inter_square/(square1 + square2 - inter_square + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def body1(num, object_num, loss, predict, labels, nilboy):\n",
    "    \"\"\"\n",
    "    calculate loss\n",
    "    Args:\n",
    "      predict: 3-D tensor [cell_size, cell_size, 5 * boxes_per_cell]\n",
    "      labels : [max_objects, 5]  (x_center, y_center, w, h, class)\n",
    "    \"\"\"\n",
    "    global image_size\n",
    "    global cell_size\n",
    "    \n",
    "    label = labels[num:num+1, :]\n",
    "    label = tf.reshape(label, [-1])\n",
    "\n",
    "    #calculate objects  tensor [CELL_SIZE, CELL_SIZE]\n",
    "    min_x = (label[0] - label[2] / 2) / (image_size / cell_size)\n",
    "    max_x = (label[0] + label[2] / 2) / (image_size / cell_size)\n",
    "\n",
    "    min_y = (label[1] - label[3] / 2) / (image_size / cell_size)\n",
    "    max_y = (label[1] + label[3] / 2) / (image_size / cell_size)\n",
    "\n",
    "    min_x = tf.floor(min_x)\n",
    "    min_y = tf.floor(min_y)\n",
    "\n",
    "    max_x = tf.ceil(max_x)\n",
    "    max_y = tf.ceil(max_y)\n",
    "\n",
    "    temp = tf.cast(tf.stack([max_y - min_y, max_x - min_x]), dtype=tf.int32)\n",
    "    objects = tf.ones(temp, tf.float32)\n",
    "\n",
    "    temp = tf.cast(tf.stack([min_y, cell_size - max_y, min_x, cell_size - max_x]), tf.int32)\n",
    "    temp = tf.reshape(temp, (2, 2))\n",
    "    objects = tf.pad(objects, temp, \"CONSTANT\")\n",
    "\n",
    "    #calculate objects  tensor [CELL_SIZE, CELL_SIZE]\n",
    "    #calculate responsible tensor [CELL_SIZE, CELL_SIZE]\n",
    "    center_x = label[0] / (image_size / cell_size)\n",
    "    center_x = tf.floor(center_x)\n",
    "\n",
    "    center_y = label[1] / (image_size / cell_size)\n",
    "    center_y = tf.floor(center_y)\n",
    "\n",
    "    response = tf.ones([1, 1], tf.float32)\n",
    "\n",
    "    temp = tf.cast(tf.stack([center_y, cell_size - center_y - 1, center_x, cell_size -center_x - 1]), tf.int32)\n",
    "    temp = tf.reshape(temp, (2, 2))\n",
    "    response = tf.pad(response, temp, \"CONSTANT\")\n",
    "    #objects = response\n",
    "\n",
    "    #calculate iou_predict_truth [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    predict_boxes = predict[:, :, num_classes + boxes_per_cell:]\n",
    "    \n",
    "\n",
    "    predict_boxes = tf.reshape(predict_boxes, [cell_size, cell_size, boxes_per_cell, 4])\n",
    "\n",
    "    predict_boxes = predict_boxes * [image_size / cell_size, image_size / cell_size, image_size, image_size]\n",
    "\n",
    "    base_boxes = np.zeros([cell_size, cell_size, 4])\n",
    "\n",
    "    for y in range(cell_size):\n",
    "        for x in range(cell_size):\n",
    "            #nilboy\n",
    "            base_boxes[y, x, :] = [image_size / cell_size * x, image_size / cell_size * y, 0, 0]\n",
    "    base_boxes = np.tile(np.resize(base_boxes, [cell_size, cell_size, 1, 4]), [1, 1, boxes_per_cell, 1])\n",
    "\n",
    "    predict_boxes = base_boxes + predict_boxes\n",
    "\n",
    "    iou_predict_truth = iou(predict_boxes, label[0:4])\n",
    "    #calculate C [cell_size, cell_size, boxes_per_cell]\n",
    "    C = iou_predict_truth * tf.reshape(response, [cell_size, cell_size, 1])\n",
    "\n",
    "    #calculate I tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    I = iou_predict_truth * tf.reshape(response, (cell_size, cell_size, 1))\n",
    "    \n",
    "    max_I = tf.reduce_max(I, 2, keep_dims=True)\n",
    "\n",
    "    I = tf.cast((I >= max_I), tf.float32) * tf.reshape(response, (cell_size, cell_size, 1))\n",
    "\n",
    "    #calculate no_I tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    no_I = tf.ones_like(I, dtype=tf.float32) - I \n",
    "\n",
    "\n",
    "    p_C = predict[:, :, num_classes:num_classes + boxes_per_cell]\n",
    "\n",
    "    #calculate truth x,y,sqrt_w,sqrt_h 0-D\n",
    "    x = label[0]\n",
    "    y = label[1]\n",
    "\n",
    "    sqrt_w = tf.sqrt(tf.abs(label[2]))\n",
    "    sqrt_h = tf.sqrt(tf.abs(label[3]))\n",
    "    #sqrt_w = tf.abs(label[2])\n",
    "    #sqrt_h = tf.abs(label[3])\n",
    "\n",
    "    #calculate predict p_x, p_y, p_sqrt_w, p_sqrt_h 3-D [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    p_x = predict_boxes[:, :, :, 0]\n",
    "    p_y = predict_boxes[:, :, :, 1]\n",
    "\n",
    "    #p_sqrt_w = tf.sqrt(tf.abs(predict_boxes[:, :, :, 2])) * ((tf.cast(predict_boxes[:, :, :, 2] > 0, tf.float32) * 2) - 1)\n",
    "    #p_sqrt_h = tf.sqrt(tf.abs(predict_boxes[:, :, :, 3])) * ((tf.cast(predict_boxes[:, :, :, 3] > 0, tf.float32) * 2) - 1)\n",
    "    #p_sqrt_w = tf.sqrt(tf.maximum(0.0, predict_boxes[:, :, :, 2]))\n",
    "    #p_sqrt_h = tf.sqrt(tf.maximum(0.0, predict_boxes[:, :, :, 3]))\n",
    "    #p_sqrt_w = predict_boxes[:, :, :, 2]\n",
    "    #p_sqrt_h = predict_boxes[:, :, :, 3]\n",
    "    p_sqrt_w = tf.sqrt(tf.minimum(image_size * 1.0, tf.maximum(0.0, predict_boxes[:, :, :, 2])))\n",
    "    p_sqrt_h = tf.sqrt(tf.minimum(image_size * 1.0, tf.maximum(0.0, predict_boxes[:, :, :, 3])))\n",
    "    #calculate truth p 1-D tensor [NUM_CLASSES]\n",
    "    P = tf.one_hot(tf.cast(label[4], tf.int32), num_classes, dtype=tf.float32)\n",
    "\n",
    "    #calculate predict p_P 3-D tensor [CELL_SIZE, CELL_SIZE, NUM_CLASSES]\n",
    "    p_P = predict[:, :, 0:num_classes]\n",
    "\n",
    "    #class_loss\n",
    "    class_loss = tf.nn.l2_loss(tf.reshape(objects, (cell_size, cell_size, 1)) * (p_P - P)) * class_scale\n",
    "    #class_loss = tf.nn.l2_loss(tf.reshape(response, (cell_size, cell_size, 1)) * (p_P - P)) * class_scale\n",
    "\n",
    "    #object_loss\n",
    "    object_loss = tf.nn.l2_loss(I * (p_C - C)) * object_scale\n",
    "    #object_loss = tf.nn.l2_loss(I * (p_C - (C + 1.0)/2.0)) * object_scale\n",
    "\n",
    "    #noobject_loss\n",
    "    #noobject_loss = tf.nn.l2_loss(no_I * (p_C - C)) * noobject_scale\n",
    "    noobject_loss = tf.nn.l2_loss(no_I * (p_C)) * noobject_scale\n",
    "\n",
    "    #coord_loss\n",
    "    coord_loss = (tf.nn.l2_loss(I * (p_x - x)/(image_size/cell_size)) +\n",
    "                 tf.nn.l2_loss(I * (p_y - y)/(image_size/cell_size)) +\n",
    "                 tf.nn.l2_loss(I * (p_sqrt_w - sqrt_w))/ image_size +\n",
    "                 tf.nn.l2_loss(I * (p_sqrt_h - sqrt_h))/image_size) * coord_scale\n",
    "\n",
    "    nilboy = I\n",
    "\n",
    "    return num + 1, object_num, [loss[0] + class_loss, loss[1] + object_loss, loss[2] + noobject_loss, loss[3] + coord_loss], predict, labels, nilboy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(predicts, labels, objects_num):\n",
    "    \"\"\"Add Loss to all the trainable variables\n",
    "\n",
    "    Args:\n",
    "      predicts: 4-D tensor [batch_size, cell_size, cell_size, 5 * boxes_per_cell]\n",
    "      ===> (num_classes, boxes_per_cell, 4 * boxes_per_cell)\n",
    "      labels  : 3-D tensor of [batch_size, max_objects, 5]\n",
    "      objects_num: 1-D tensor [batch_size]\n",
    "    \"\"\"\n",
    "    class_loss = tf.constant(0, tf.float32)\n",
    "    object_loss = tf.constant(0, tf.float32)\n",
    "    noobject_loss = tf.constant(0, tf.float32)\n",
    "    coord_loss = tf.constant(0, tf.float32)\n",
    "    loss = [0, 0, 0, 0]\n",
    "    for i in range(batch_size):\n",
    "        predict = predicts[i, :, :, :]\n",
    "        label = labels[i, :, :]\n",
    "        object_num = objects_num[i]\n",
    "        nilboy = tf.ones([7,7,2])\n",
    "        tuple_results = tf.while_loop(cond1, body1, [tf.constant(0), object_num, [class_loss, object_loss, noobject_loss, coord_loss], predict, label, nilboy])\n",
    "        for j in range(4):\n",
    "            loss[j] = loss[j] + tuple_results[2][j]\n",
    "        nilboy = tuple_results[5]\n",
    "\n",
    "    tf.add_to_collection('losses', (loss[0] + loss[1] + loss[2] + loss[3]) / batch_size)\n",
    "\n",
    "#     tf.summary.scalar('class_loss', loss[0]/self.batch_size)\n",
    "#     tf.summary.scalar('object_loss', loss[1]/self.batch_size)\n",
    "#     tf.summary.scalar('noobject_loss', loss[2]/self.batch_size)\n",
    "#     tf.summary.scalar('coord_loss', loss[3]/self.batch_size)\n",
    "#     tf.summary.scalar('weight_loss', tf.add_n(tf.get_collection('losses')) - (loss[0] + loss[1] + loss[2] + loss[3])/self.batch_size )\n",
    "\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss'), nilboy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_size = 448\n",
    "batch_size = 16\n",
    "num_classes = 20\n",
    "max_objects_per_image = 20\n",
    "width = image_size\n",
    "height = image_size\n",
    "max_objects = max_objects_per_image\n",
    "cell_size = 7\n",
    "boxes_per_cell = 2\n",
    "object_scale = 1\n",
    "noobject_scale = 0.5\n",
    "class_scale = 1\n",
    "coord_scale = 5\n",
    "learning_rate = 0.000001\n",
    "max_iterators = 10000\n",
    "\n",
    "images = tf.placeholder(tf.float32, (None, height, width, 3))\n",
    "labels = tf.placeholder(tf.float32, (None, max_objects, 5))\n",
    "objects_num = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "conv_num = 1\n",
    "\n",
    "temp_conv = conv2d('conv' + str(conv_num), images, [3, 3, 3, 16], stride=1)\n",
    "conv_num += 1\n",
    "\n",
    "temp_pool = max_pool(temp_conv, [2, 2], 2)\n",
    "\n",
    "temp_conv = conv2d('conv' + str(conv_num), temp_pool, [3, 3, 16, 32], stride=1)\n",
    "conv_num += 1\n",
    "\n",
    "temp_pool = max_pool(temp_conv, [2, 2], 2)\n",
    "\n",
    "temp_conv = conv2d('conv' + str(conv_num), temp_pool, [3, 3, 32, 64], stride=1)\n",
    "conv_num += 1\n",
    "\n",
    "temp_conv = max_pool(temp_conv, [2, 2], 2)\n",
    "\n",
    "temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 64, 128], stride=1)\n",
    "conv_num += 1\n",
    "\n",
    "temp_conv = max_pool(temp_conv, [2, 2], 2)\n",
    "\n",
    "temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 128, 256], stride=1)\n",
    "conv_num += 1\n",
    "\n",
    "temp_conv = max_pool(temp_conv, [2, 2], 2)\n",
    "\n",
    "temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 256, 512], stride=1)\n",
    "conv_num += 1\n",
    "\n",
    "temp_conv = max_pool(temp_conv, [2, 2], 2)\n",
    "\n",
    "temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 512, 1024], stride=1)\n",
    "conv_num += 1     \n",
    "\n",
    "temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 1024, 1024], stride=1)\n",
    "conv_num += 1 \n",
    "\n",
    "temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 1024, 1024], stride=1)\n",
    "conv_num += 1 \n",
    "\n",
    "temp_conv = tf.transpose(temp_conv, (0, 3, 1, 2))\n",
    "\n",
    "#Fully connected layer\n",
    "local1 = local('local1', temp_conv, cell_size * cell_size * 1024, 256)\n",
    "\n",
    "local2 = local('local2', local1, 256, 4096)\n",
    "\n",
    "local3 = local('local3', local2, 4096, cell_size * cell_size * (num_classes + boxes_per_cell * 5), leaky=False)\n",
    "\n",
    "n1 = cell_size * cell_size * num_classes\n",
    "\n",
    "n2 = n1 + cell_size * cell_size * boxes_per_cell\n",
    "\n",
    "class_probs = tf.reshape(local3[:, 0:n1], (-1, cell_size, cell_size, num_classes))\n",
    "scales = tf.reshape(local3[:, n1:n2], (-1, cell_size, cell_size, boxes_per_cell))\n",
    "boxes = tf.reshape(local3[:, n2:], (-1, cell_size, cell_size, boxes_per_cell * 4))\n",
    "\n",
    "# local3 = tf.concat([class_probs, scales, boxes], 3)\n",
    "\n",
    "# n1 = cell_size * cell_size * num_classes\n",
    "\n",
    "# n2 = n1 + cell_size * cell_size * boxes_per_cell\n",
    "\n",
    "# class_probs = tf.reshape(local3[:, 0:n1], (-1, cell_size, cell_size, num_classes))\n",
    "# scales = tf.reshape(local3[:, n1:n2], (-1, cell_size, cell_size, boxes_per_cell))\n",
    "# boxes = tf.reshape(local3[:, n2:], (-1, cell_size, cell_size, boxes_per_cell * 4))\n",
    "\n",
    "predicts = tf.concat([class_probs, scales, boxes], 3)\n",
    "\n",
    "total_loss, nilboy = loss(predicts, labels, objects_num)\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(total_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cvtColor(image):\n",
    "    arr = np.array(image)\n",
    "    tmp = arr[:, :, 0].copy()\n",
    "    arr[:, :, 0] = arr[:, :, 2]\n",
    "    arr[:, :, 2] = tmp\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def record_process(record):\n",
    "    \"\"\"record process \n",
    "    Args: record \n",
    "    Returns:\n",
    "      image: 3-D ndarray\n",
    "      labels: 2-D list [self.max_objects, 5] (xcenter, ycenter, w, h, class_num)\n",
    "      object_num:  total object number  int \n",
    "    \"\"\"\n",
    "    global width\n",
    "    global height\n",
    "    global max_objects\n",
    "    \n",
    "    image = Image.open(record[0])\n",
    "    image = cvtColor(image)\n",
    "#     image = cv2.imread(record[0])\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]\n",
    "\n",
    "    width_rate = width * 1.0 / w \n",
    "    height_rate = height * 1.0 / h \n",
    "    \n",
    "    image = Image.fromarray(image)\n",
    "    image = image.resize((height, width))\n",
    "#     image = cv2.resize(image, (height, width))\n",
    "\n",
    "    labels = [[0, 0, 0, 0, 0]] * max_objects\n",
    "    i = 1\n",
    "    object_num = 0\n",
    "    while i < len(record):\n",
    "        xmin = record[i]\n",
    "        ymin = record[i + 1]\n",
    "        xmax = record[i + 2]\n",
    "        ymax = record[i + 3]\n",
    "        class_num = record[i + 4]\n",
    "\n",
    "        xcenter = (xmin + xmax) * 1.0 / 2 * width_rate\n",
    "        ycenter = (ymin + ymax) * 1.0 / 2 * height_rate\n",
    "\n",
    "        box_w = (xmax - xmin) * width_rate\n",
    "        box_h = (ymax - ymin) * height_rate\n",
    "\n",
    "        labels[object_num] = [xcenter, ycenter, box_w, box_h, class_num]\n",
    "        object_num += 1\n",
    "        i += 5\n",
    "        if object_num >= max_objects:\n",
    "            break\n",
    "    return image, labels, object_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'yolo2d_data/pascal_voc.txt'\n",
    "record_list = []  \n",
    "\n",
    "# filling the record_list\n",
    "input_file = open(data_path, 'r')\n",
    "\n",
    "for line in input_file:\n",
    "    line = line.strip()\n",
    "    ss = line.split(' ')\n",
    "    ss[1:] = [float(num) for num in ss[1:]]\n",
    "    record_list.append(ss)\n",
    "\n",
    "record_point = 0\n",
    "record_number = len(record_list)\n",
    "num_batch_per_epoch = int(np.ceil(record_number / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from yolo2d_model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saveDir = 'yolo2d_model'\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, saveDir + '/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init =  tf.global_variables_initializer()\n",
    "# sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-19 09:26:03.798773: step 0 batch 0, loss = 13.53 (27.4 examples/sec; 0.583 sec/batch)\n",
      "2017-06-19 09:26:09.166622: step 0 batch 10, loss = 10.61 (2.7 examples/sec; 5.951 sec/batch)\n",
      "2017-06-19 09:26:14.470197: step 0 batch 20, loss = 14.24 (1.4 examples/sec; 11.255 sec/batch)\n",
      "2017-06-19 09:26:19.789655: step 0 batch 30, loss = 19.08 (1.0 examples/sec; 16.574 sec/batch)\n",
      "2017-06-19 09:26:25.034771: step 0 batch 40, loss = 12.76 (0.7 examples/sec; 21.819 sec/batch)\n",
      "2017-06-19 09:26:30.447939: step 0 batch 50, loss = 17.76 (0.6 examples/sec; 27.233 sec/batch)\n",
      "2017-06-19 09:26:35.845638: step 0 batch 60, loss = 19.15 (0.5 examples/sec; 32.630 sec/batch)\n",
      "2017-06-19 09:26:41.085794: step 0 batch 70, loss = 11.49 (0.4 examples/sec; 37.870 sec/batch)\n",
      "2017-06-19 09:26:46.373851: step 0 batch 80, loss = 13.87 (0.4 examples/sec; 43.158 sec/batch)\n",
      "2017-06-19 09:26:51.656256: step 0 batch 90, loss = 14.95 (0.3 examples/sec; 48.441 sec/batch)\n",
      "2017-06-19 09:26:56.766537: step 0 batch 100, loss = 12.18 (0.3 examples/sec; 53.551 sec/batch)\n",
      "2017-06-19 09:27:02.170452: step 0 batch 110, loss = 16.39 (0.3 examples/sec; 58.955 sec/batch)\n",
      "2017-06-19 09:27:07.525219: step 0 batch 120, loss = 14.11 (0.2 examples/sec; 64.310 sec/batch)\n",
      "2017-06-19 09:27:12.930307: step 0 batch 130, loss = 14.05 (0.2 examples/sec; 69.715 sec/batch)\n",
      "2017-06-19 09:27:18.163954: step 0 batch 140, loss = 14.47 (0.2 examples/sec; 74.949 sec/batch)\n",
      "2017-06-19 09:27:23.497595: step 0 batch 150, loss = 15.27 (0.2 examples/sec; 80.282 sec/batch)\n",
      "2017-06-19 09:27:28.805086: step 0 batch 160, loss = 12.60 (0.2 examples/sec; 85.590 sec/batch)\n",
      "2017-06-19 09:27:34.264697: step 0 batch 170, loss = 15.99 (0.2 examples/sec; 91.049 sec/batch)\n",
      "2017-06-19 09:27:39.681374: step 0 batch 180, loss = 10.72 (0.2 examples/sec; 96.466 sec/batch)\n",
      "2017-06-19 09:27:45.041326: step 0 batch 190, loss = 13.20 (0.2 examples/sec; 101.826 sec/batch)\n",
      "2017-06-19 09:27:50.298826: step 0 batch 200, loss = 12.92 (0.1 examples/sec; 107.083 sec/batch)\n",
      "2017-06-19 09:27:55.630142: step 0 batch 210, loss = 13.39 (0.1 examples/sec; 112.415 sec/batch)\n",
      "2017-06-19 09:28:00.974677: step 0 batch 220, loss = 18.26 (0.1 examples/sec; 117.759 sec/batch)\n",
      "2017-06-19 09:28:06.242624: step 0 batch 230, loss = 10.29 (0.1 examples/sec; 123.027 sec/batch)\n",
      "2017-06-19 09:28:11.666095: step 0 batch 240, loss = 10.90 (0.1 examples/sec; 128.451 sec/batch)\n",
      "2017-06-19 09:28:16.923436: step 0 batch 250, loss = 9.70 (0.1 examples/sec; 133.708 sec/batch)\n",
      "2017-06-19 09:28:22.243417: step 0 batch 260, loss = 13.31 (0.1 examples/sec; 139.028 sec/batch)\n",
      "2017-06-19 09:28:27.352166: step 0 batch 270, loss = 12.69 (0.1 examples/sec; 144.137 sec/batch)\n",
      "2017-06-19 09:28:32.686494: step 0 batch 280, loss = 14.36 (0.1 examples/sec; 149.471 sec/batch)\n",
      "2017-06-19 09:28:37.962620: step 0 batch 290, loss = 11.91 (0.1 examples/sec; 154.747 sec/batch)\n",
      "2017-06-19 09:28:43.218340: step 0 batch 300, loss = 17.16 (0.1 examples/sec; 160.003 sec/batch)\n",
      "2017-06-19 09:28:48.405115: step 0 batch 310, loss = 13.40 (0.1 examples/sec; 165.190 sec/batch)\n",
      "2017-06-19 09:28:53.754364: step 0 batch 320, loss = 16.22 (0.1 examples/sec; 170.539 sec/batch)\n",
      "2017-06-19 09:28:59.115094: step 0 batch 330, loss = 8.47 (0.1 examples/sec; 175.900 sec/batch)\n",
      "2017-06-19 09:29:04.435895: step 0 batch 340, loss = 12.57 (0.1 examples/sec; 181.220 sec/batch)\n",
      "2017-06-19 09:29:09.779338: step 0 batch 350, loss = 14.14 (0.1 examples/sec; 186.564 sec/batch)\n",
      "2017-06-19 09:29:15.075375: step 0 batch 360, loss = 12.49 (0.1 examples/sec; 191.860 sec/batch)\n",
      "2017-06-19 09:29:20.314011: step 0 batch 370, loss = 15.08 (0.1 examples/sec; 197.099 sec/batch)\n",
      "2017-06-19 09:29:25.477968: step 0 batch 380, loss = 11.52 (0.1 examples/sec; 202.263 sec/batch)\n",
      "2017-06-19 09:29:30.771657: step 0 batch 390, loss = 12.14 (0.1 examples/sec; 207.556 sec/batch)\n",
      "2017-06-19 09:29:36.059133: step 0 batch 400, loss = 12.11 (0.1 examples/sec; 212.844 sec/batch)\n",
      "2017-06-19 09:29:41.344539: step 0 batch 410, loss = 13.83 (0.1 examples/sec; 218.129 sec/batch)\n",
      "2017-06-19 09:29:46.654129: step 0 batch 420, loss = 12.40 (0.1 examples/sec; 223.439 sec/batch)\n",
      "2017-06-19 09:29:52.031692: step 0 batch 430, loss = 11.32 (0.1 examples/sec; 228.816 sec/batch)\n",
      "2017-06-19 09:29:57.242186: step 0 batch 440, loss = 12.51 (0.1 examples/sec; 234.027 sec/batch)\n",
      "2017-06-19 09:30:02.644069: step 0 batch 450, loss = 13.42 (0.1 examples/sec; 239.429 sec/batch)\n",
      "2017-06-19 09:30:08.019694: step 0 batch 460, loss = 9.87 (0.1 examples/sec; 244.804 sec/batch)\n",
      "2017-06-19 09:30:13.302702: step 0 batch 470, loss = 13.68 (0.1 examples/sec; 250.087 sec/batch)\n",
      "2017-06-19 09:30:18.656032: step 0 batch 480, loss = 11.39 (0.1 examples/sec; 255.441 sec/batch)\n",
      "2017-06-19 09:30:23.961053: step 0 batch 490, loss = 12.32 (0.1 examples/sec; 260.746 sec/batch)\n",
      "2017-06-19 09:30:29.251882: step 0 batch 500, loss = 15.67 (0.1 examples/sec; 266.036 sec/batch)\n",
      "2017-06-19 09:30:34.520111: step 0 batch 510, loss = 11.71 (0.1 examples/sec; 271.305 sec/batch)\n",
      "2017-06-19 09:30:39.748650: step 0 batch 520, loss = 16.56 (0.1 examples/sec; 276.533 sec/batch)\n",
      "2017-06-19 09:30:45.166832: step 0 batch 530, loss = 10.52 (0.1 examples/sec; 281.951 sec/batch)\n",
      "2017-06-19 09:30:50.358183: step 0 batch 540, loss = 19.58 (0.1 examples/sec; 287.143 sec/batch)\n",
      "2017-06-19 09:30:55.608822: step 0 batch 550, loss = 15.05 (0.1 examples/sec; 292.393 sec/batch)\n",
      "2017-06-19 09:31:01.018904: step 0 batch 560, loss = 16.67 (0.1 examples/sec; 297.803 sec/batch)\n",
      "2017-06-19 09:31:06.301534: step 0 batch 570, loss = 15.13 (0.1 examples/sec; 303.086 sec/batch)\n",
      "2017-06-19 09:31:11.501047: step 0 batch 580, loss = 11.94 (0.1 examples/sec; 308.286 sec/batch)\n",
      "2017-06-19 09:31:16.899477: step 0 batch 590, loss = 12.68 (0.1 examples/sec; 313.684 sec/batch)\n",
      "2017-06-19 09:31:22.176187: step 0 batch 600, loss = 11.67 (0.1 examples/sec; 318.961 sec/batch)\n",
      "2017-06-19 09:31:27.505261: step 0 batch 610, loss = 14.87 (0.0 examples/sec; 324.290 sec/batch)\n",
      "2017-06-19 09:31:30.528960: step 1 batch 0, loss = 11.25 (29.9 examples/sec; 0.535 sec/batch)\n",
      "2017-06-19 09:31:35.704422: step 1 batch 10, loss = 15.98 (2.8 examples/sec; 5.710 sec/batch)\n",
      "2017-06-19 09:31:41.051768: step 1 batch 20, loss = 11.41 (1.4 examples/sec; 11.057 sec/batch)\n",
      "2017-06-19 09:31:46.371790: step 1 batch 30, loss = 10.91 (1.0 examples/sec; 16.377 sec/batch)\n",
      "2017-06-19 09:31:51.758450: step 1 batch 40, loss = 16.23 (0.7 examples/sec; 21.764 sec/batch)\n",
      "2017-06-19 09:31:57.127147: step 1 batch 50, loss = 14.76 (0.6 examples/sec; 27.133 sec/batch)\n",
      "2017-06-19 09:32:02.265493: step 1 batch 60, loss = 12.59 (0.5 examples/sec; 32.271 sec/batch)\n",
      "2017-06-19 09:32:07.615963: step 1 batch 70, loss = 14.83 (0.4 examples/sec; 37.622 sec/batch)\n",
      "2017-06-19 09:32:12.897472: step 1 batch 80, loss = 12.05 (0.4 examples/sec; 42.903 sec/batch)\n",
      "2017-06-19 09:32:18.084754: step 1 batch 90, loss = 13.68 (0.3 examples/sec; 48.090 sec/batch)\n",
      "2017-06-19 09:32:23.381112: step 1 batch 100, loss = 11.46 (0.3 examples/sec; 53.387 sec/batch)\n",
      "2017-06-19 09:32:28.651250: step 1 batch 110, loss = 12.05 (0.3 examples/sec; 58.657 sec/batch)\n",
      "2017-06-19 09:32:34.028010: step 1 batch 120, loss = 9.26 (0.2 examples/sec; 64.034 sec/batch)\n",
      "2017-06-19 09:32:39.211351: step 1 batch 130, loss = 13.48 (0.2 examples/sec; 69.217 sec/batch)\n",
      "2017-06-19 09:32:44.541956: step 1 batch 140, loss = 12.64 (0.2 examples/sec; 74.548 sec/batch)\n",
      "2017-06-19 09:32:49.910556: step 1 batch 150, loss = 10.55 (0.2 examples/sec; 79.916 sec/batch)\n",
      "2017-06-19 09:32:55.261087: step 1 batch 160, loss = 13.00 (0.2 examples/sec; 85.267 sec/batch)\n",
      "2017-06-19 09:33:00.486175: step 1 batch 170, loss = 15.45 (0.2 examples/sec; 90.492 sec/batch)\n",
      "2017-06-19 09:33:05.712785: step 1 batch 180, loss = 10.21 (0.2 examples/sec; 95.718 sec/batch)\n",
      "2017-06-19 09:33:10.940500: step 1 batch 190, loss = 17.57 (0.2 examples/sec; 100.946 sec/batch)\n",
      "2017-06-19 09:33:16.251796: step 1 batch 200, loss = 17.17 (0.2 examples/sec; 106.257 sec/batch)\n",
      "2017-06-19 09:33:21.559459: step 1 batch 210, loss = 10.63 (0.1 examples/sec; 111.565 sec/batch)\n",
      "2017-06-19 09:33:27.068421: step 1 batch 220, loss = 15.92 (0.1 examples/sec; 117.074 sec/batch)\n",
      "2017-06-19 09:33:32.362962: step 1 batch 230, loss = 11.04 (0.1 examples/sec; 122.369 sec/batch)\n",
      "2017-06-19 09:33:37.790111: step 1 batch 240, loss = 14.54 (0.1 examples/sec; 127.796 sec/batch)\n",
      "2017-06-19 09:33:43.071801: step 1 batch 250, loss = 14.39 (0.1 examples/sec; 133.077 sec/batch)\n",
      "2017-06-19 09:33:48.283251: step 1 batch 260, loss = 16.78 (0.1 examples/sec; 138.289 sec/batch)\n",
      "2017-06-19 09:33:53.738793: step 1 batch 270, loss = 14.45 (0.1 examples/sec; 143.744 sec/batch)\n",
      "2017-06-19 09:33:58.943870: step 1 batch 280, loss = 11.30 (0.1 examples/sec; 148.949 sec/batch)\n",
      "2017-06-19 09:34:04.274191: step 1 batch 290, loss = 12.40 (0.1 examples/sec; 154.280 sec/batch)\n",
      "2017-06-19 09:34:09.444690: step 1 batch 300, loss = 10.17 (0.1 examples/sec; 159.450 sec/batch)\n",
      "2017-06-19 09:34:14.665756: step 1 batch 310, loss = 10.43 (0.1 examples/sec; 164.671 sec/batch)\n",
      "2017-06-19 09:34:20.051313: step 1 batch 320, loss = 9.84 (0.1 examples/sec; 170.057 sec/batch)\n",
      "2017-06-19 09:34:25.281975: step 1 batch 330, loss = 12.07 (0.1 examples/sec; 175.288 sec/batch)\n",
      "2017-06-19 09:34:30.571649: step 1 batch 340, loss = 12.73 (0.1 examples/sec; 180.577 sec/batch)\n",
      "2017-06-19 09:34:35.840160: step 1 batch 350, loss = 8.14 (0.1 examples/sec; 185.846 sec/batch)\n",
      "2017-06-19 09:34:41.137858: step 1 batch 360, loss = 12.17 (0.1 examples/sec; 191.143 sec/batch)\n",
      "2017-06-19 09:34:46.490908: step 1 batch 370, loss = 8.71 (0.1 examples/sec; 196.496 sec/batch)\n",
      "2017-06-19 09:34:51.766428: step 1 batch 380, loss = 12.56 (0.1 examples/sec; 201.772 sec/batch)\n",
      "2017-06-19 09:34:56.966712: step 1 batch 390, loss = 10.40 (0.1 examples/sec; 206.972 sec/batch)\n",
      "2017-06-19 09:35:02.190332: step 1 batch 400, loss = 11.67 (0.1 examples/sec; 212.196 sec/batch)\n",
      "2017-06-19 09:35:07.601131: step 1 batch 410, loss = 10.53 (0.1 examples/sec; 217.607 sec/batch)\n",
      "2017-06-19 09:35:12.803700: step 1 batch 420, loss = 12.84 (0.1 examples/sec; 222.809 sec/batch)\n",
      "2017-06-19 09:35:18.180669: step 1 batch 430, loss = 13.45 (0.1 examples/sec; 228.186 sec/batch)\n",
      "2017-06-19 09:35:23.404782: step 1 batch 440, loss = 12.97 (0.1 examples/sec; 233.410 sec/batch)\n",
      "2017-06-19 09:35:28.666837: step 1 batch 450, loss = 15.70 (0.1 examples/sec; 238.672 sec/batch)\n",
      "2017-06-19 09:35:33.854931: step 1 batch 460, loss = 13.30 (0.1 examples/sec; 243.861 sec/batch)\n",
      "2017-06-19 09:35:39.220789: step 1 batch 470, loss = 15.72 (0.1 examples/sec; 249.226 sec/batch)\n",
      "2017-06-19 09:35:44.548225: step 1 batch 480, loss = 13.28 (0.1 examples/sec; 254.554 sec/batch)\n",
      "2017-06-19 09:35:49.896343: step 1 batch 490, loss = 12.26 (0.1 examples/sec; 259.902 sec/batch)\n",
      "2017-06-19 09:35:55.300164: step 1 batch 500, loss = 9.53 (0.1 examples/sec; 265.306 sec/batch)\n",
      "2017-06-19 09:36:00.542843: step 1 batch 510, loss = 14.95 (0.1 examples/sec; 270.548 sec/batch)\n",
      "2017-06-19 09:36:05.767593: step 1 batch 520, loss = 14.74 (0.1 examples/sec; 275.773 sec/batch)\n",
      "2017-06-19 09:36:11.058555: step 1 batch 530, loss = 15.66 (0.1 examples/sec; 281.064 sec/batch)\n",
      "2017-06-19 09:36:16.434113: step 1 batch 540, loss = 10.84 (0.1 examples/sec; 286.440 sec/batch)\n",
      "2017-06-19 09:36:21.733503: step 1 batch 550, loss = 12.84 (0.1 examples/sec; 291.739 sec/batch)\n",
      "2017-06-19 09:36:27.122502: step 1 batch 560, loss = 9.51 (0.1 examples/sec; 297.128 sec/batch)\n",
      "2017-06-19 09:36:32.399779: step 1 batch 570, loss = 16.25 (0.1 examples/sec; 302.405 sec/batch)\n",
      "2017-06-19 09:36:37.777017: step 1 batch 580, loss = 14.21 (0.1 examples/sec; 307.783 sec/batch)\n",
      "2017-06-19 09:36:43.245549: step 1 batch 590, loss = 12.79 (0.1 examples/sec; 313.251 sec/batch)\n",
      "2017-06-19 09:36:48.504681: step 1 batch 600, loss = 11.98 (0.1 examples/sec; 318.510 sec/batch)\n",
      "2017-06-19 09:36:53.890965: step 1 batch 610, loss = 14.25 (0.0 examples/sec; 323.897 sec/batch)\n",
      "2017-06-19 09:36:57.047496: step 2 batch 0, loss = 13.17 (29.3 examples/sec; 0.547 sec/batch)\n",
      "2017-06-19 09:37:02.448329: step 2 batch 10, loss = 12.00 (2.7 examples/sec; 5.948 sec/batch)\n",
      "2017-06-19 09:37:07.796603: step 2 batch 20, loss = 11.60 (1.4 examples/sec; 11.296 sec/batch)\n",
      "2017-06-19 09:37:13.132329: step 2 batch 30, loss = 10.73 (1.0 examples/sec; 16.632 sec/batch)\n",
      "2017-06-19 09:37:18.418330: step 2 batch 40, loss = 11.75 (0.7 examples/sec; 21.918 sec/batch)\n",
      "2017-06-19 09:37:23.619843: step 2 batch 50, loss = 10.42 (0.6 examples/sec; 27.119 sec/batch)\n",
      "2017-06-19 09:37:28.854120: step 2 batch 60, loss = 12.72 (0.5 examples/sec; 32.353 sec/batch)\n",
      "2017-06-19 09:37:34.123524: step 2 batch 70, loss = 14.96 (0.4 examples/sec; 37.623 sec/batch)\n",
      "2017-06-19 09:37:39.562433: step 2 batch 80, loss = 15.86 (0.4 examples/sec; 43.062 sec/batch)\n",
      "2017-06-19 09:37:44.800530: step 2 batch 90, loss = 11.38 (0.3 examples/sec; 48.300 sec/batch)\n",
      "2017-06-19 09:37:50.283214: step 2 batch 100, loss = 13.67 (0.3 examples/sec; 53.783 sec/batch)\n",
      "2017-06-19 09:37:55.814006: step 2 batch 110, loss = 12.94 (0.3 examples/sec; 59.313 sec/batch)\n",
      "2017-06-19 09:38:00.977619: step 2 batch 120, loss = 10.72 (0.2 examples/sec; 64.477 sec/batch)\n",
      "2017-06-19 09:38:06.225787: step 2 batch 130, loss = 13.04 (0.2 examples/sec; 69.725 sec/batch)\n",
      "2017-06-19 09:38:11.406681: step 2 batch 140, loss = 13.22 (0.2 examples/sec; 74.906 sec/batch)\n",
      "2017-06-19 09:38:16.691001: step 2 batch 150, loss = 13.14 (0.2 examples/sec; 80.190 sec/batch)\n",
      "2017-06-19 09:38:21.877047: step 2 batch 160, loss = 11.75 (0.2 examples/sec; 85.376 sec/batch)\n",
      "2017-06-19 09:38:27.138923: step 2 batch 170, loss = 17.73 (0.2 examples/sec; 90.638 sec/batch)\n",
      "2017-06-19 09:38:32.529770: step 2 batch 180, loss = 12.32 (0.2 examples/sec; 96.029 sec/batch)\n",
      "2017-06-19 09:38:37.865609: step 2 batch 190, loss = 7.13 (0.2 examples/sec; 101.365 sec/batch)\n",
      "2017-06-19 09:38:43.277858: step 2 batch 200, loss = 13.56 (0.1 examples/sec; 106.777 sec/batch)\n",
      "2017-06-19 09:38:48.524613: step 2 batch 210, loss = 14.20 (0.1 examples/sec; 112.024 sec/batch)\n",
      "2017-06-19 09:38:53.830027: step 2 batch 220, loss = 14.00 (0.1 examples/sec; 117.329 sec/batch)\n",
      "2017-06-19 09:38:59.274333: step 2 batch 230, loss = 15.72 (0.1 examples/sec; 122.774 sec/batch)\n",
      "2017-06-19 09:39:04.487966: step 2 batch 240, loss = 10.42 (0.1 examples/sec; 127.987 sec/batch)\n",
      "2017-06-19 09:39:09.658765: step 2 batch 250, loss = 12.81 (0.1 examples/sec; 133.158 sec/batch)\n",
      "2017-06-19 09:39:15.262818: step 2 batch 260, loss = 13.64 (0.1 examples/sec; 138.762 sec/batch)\n",
      "2017-06-19 09:39:20.520738: step 2 batch 270, loss = 9.60 (0.1 examples/sec; 144.020 sec/batch)\n",
      "2017-06-19 09:39:25.822882: step 2 batch 280, loss = 10.34 (0.1 examples/sec; 149.322 sec/batch)\n",
      "2017-06-19 09:39:31.019211: step 2 batch 290, loss = 13.02 (0.1 examples/sec; 154.519 sec/batch)\n",
      "2017-06-19 09:39:36.463572: step 2 batch 300, loss = 12.60 (0.1 examples/sec; 159.963 sec/batch)\n",
      "2017-06-19 09:39:41.607283: step 2 batch 310, loss = 11.11 (0.1 examples/sec; 165.107 sec/batch)\n",
      "2017-06-19 09:39:46.954598: step 2 batch 320, loss = 12.64 (0.1 examples/sec; 170.454 sec/batch)\n",
      "2017-06-19 09:39:52.273654: step 2 batch 330, loss = 14.19 (0.1 examples/sec; 175.773 sec/batch)\n",
      "2017-06-19 09:39:57.503016: step 2 batch 340, loss = 14.36 (0.1 examples/sec; 181.002 sec/batch)\n",
      "2017-06-19 09:40:02.670770: step 2 batch 350, loss = 11.38 (0.1 examples/sec; 186.170 sec/batch)\n",
      "2017-06-19 09:40:07.935838: step 2 batch 360, loss = 10.15 (0.1 examples/sec; 191.435 sec/batch)\n",
      "2017-06-19 09:40:13.236578: step 2 batch 370, loss = 16.50 (0.1 examples/sec; 196.736 sec/batch)\n",
      "2017-06-19 09:40:18.623648: step 2 batch 380, loss = 12.99 (0.1 examples/sec; 202.123 sec/batch)\n",
      "2017-06-19 09:40:23.991072: step 2 batch 390, loss = 12.30 (0.1 examples/sec; 207.490 sec/batch)\n",
      "2017-06-19 09:40:29.339788: step 2 batch 400, loss = 12.14 (0.1 examples/sec; 212.839 sec/batch)\n",
      "2017-06-19 09:40:34.654632: step 2 batch 410, loss = 10.76 (0.1 examples/sec; 218.154 sec/batch)\n",
      "2017-06-19 09:40:39.944361: step 2 batch 420, loss = 12.37 (0.1 examples/sec; 223.444 sec/batch)\n",
      "2017-06-19 09:40:45.203267: step 2 batch 430, loss = 12.38 (0.1 examples/sec; 228.703 sec/batch)\n",
      "2017-06-19 09:40:50.495630: step 2 batch 440, loss = 11.47 (0.1 examples/sec; 233.995 sec/batch)\n",
      "2017-06-19 09:40:55.816652: step 2 batch 450, loss = 11.31 (0.1 examples/sec; 239.316 sec/batch)\n",
      "2017-06-19 09:41:01.170368: step 2 batch 460, loss = 12.36 (0.1 examples/sec; 244.670 sec/batch)\n",
      "2017-06-19 09:41:06.399183: step 2 batch 470, loss = 11.21 (0.1 examples/sec; 249.899 sec/batch)\n",
      "2017-06-19 09:41:11.558036: step 2 batch 480, loss = 13.38 (0.1 examples/sec; 255.057 sec/batch)\n",
      "2017-06-19 09:41:17.089890: step 2 batch 490, loss = 14.22 (0.1 examples/sec; 260.589 sec/batch)\n",
      "2017-06-19 09:41:22.432909: step 2 batch 500, loss = 15.75 (0.1 examples/sec; 265.932 sec/batch)\n",
      "2017-06-19 09:41:27.631868: step 2 batch 510, loss = 12.83 (0.1 examples/sec; 271.131 sec/batch)\n",
      "2017-06-19 09:41:33.150600: step 2 batch 520, loss = 19.78 (0.1 examples/sec; 276.650 sec/batch)\n",
      "2017-06-19 09:41:38.424973: step 2 batch 530, loss = 14.30 (0.1 examples/sec; 281.924 sec/batch)\n",
      "2017-06-19 09:41:43.581386: step 2 batch 540, loss = 12.76 (0.1 examples/sec; 287.081 sec/batch)\n",
      "2017-06-19 09:41:48.840410: step 2 batch 550, loss = 12.60 (0.1 examples/sec; 292.340 sec/batch)\n",
      "2017-06-19 09:41:54.109182: step 2 batch 560, loss = 14.42 (0.1 examples/sec; 297.609 sec/batch)\n",
      "2017-06-19 09:41:59.490298: step 2 batch 570, loss = 15.07 (0.1 examples/sec; 302.990 sec/batch)\n",
      "2017-06-19 09:42:04.861980: step 2 batch 580, loss = 12.09 (0.1 examples/sec; 308.361 sec/batch)\n",
      "2017-06-19 09:42:10.083749: step 2 batch 590, loss = 13.16 (0.1 examples/sec; 313.583 sec/batch)\n",
      "2017-06-19 09:42:15.335319: step 2 batch 600, loss = 15.07 (0.1 examples/sec; 318.835 sec/batch)\n",
      "2017-06-19 09:42:20.709237: step 2 batch 610, loss = 12.62 (0.0 examples/sec; 324.209 sec/batch)\n",
      "2017-06-19 09:42:23.863750: step 3 batch 0, loss = 10.63 (32.3 examples/sec; 0.495 sec/batch)\n",
      "2017-06-19 09:42:28.977054: step 3 batch 10, loss = 13.26 (2.9 examples/sec; 5.609 sec/batch)\n",
      "2017-06-19 09:42:34.319521: step 3 batch 20, loss = 12.72 (1.5 examples/sec; 10.951 sec/batch)\n",
      "2017-06-19 09:42:39.513832: step 3 batch 30, loss = 10.94 (1.0 examples/sec; 16.145 sec/batch)\n",
      "2017-06-19 09:42:44.843883: step 3 batch 40, loss = 12.94 (0.7 examples/sec; 21.476 sec/batch)\n",
      "2017-06-19 09:42:50.369123: step 3 batch 50, loss = 14.21 (0.6 examples/sec; 27.001 sec/batch)\n",
      "2017-06-19 09:42:55.652613: step 3 batch 60, loss = 12.87 (0.5 examples/sec; 32.284 sec/batch)\n",
      "2017-06-19 09:43:00.858085: step 3 batch 70, loss = 10.06 (0.4 examples/sec; 37.490 sec/batch)\n",
      "2017-06-19 09:43:06.193998: step 3 batch 80, loss = 16.39 (0.4 examples/sec; 42.826 sec/batch)\n",
      "2017-06-19 09:43:11.524635: step 3 batch 90, loss = 13.81 (0.3 examples/sec; 48.156 sec/batch)\n",
      "2017-06-19 09:43:16.822632: step 3 batch 100, loss = 15.83 (0.3 examples/sec; 53.454 sec/batch)\n",
      "2017-06-19 09:43:22.148711: step 3 batch 110, loss = 13.30 (0.3 examples/sec; 58.780 sec/batch)\n",
      "2017-06-19 09:43:27.287539: step 3 batch 120, loss = 14.11 (0.3 examples/sec; 63.919 sec/batch)\n",
      "2017-06-19 09:43:32.570488: step 3 batch 130, loss = 16.46 (0.2 examples/sec; 69.202 sec/batch)\n",
      "2017-06-19 09:43:37.828736: step 3 batch 140, loss = 12.24 (0.2 examples/sec; 74.460 sec/batch)\n",
      "2017-06-19 09:43:43.111996: step 3 batch 150, loss = 12.93 (0.2 examples/sec; 79.744 sec/batch)\n",
      "2017-06-19 09:43:48.311883: step 3 batch 160, loss = 12.02 (0.2 examples/sec; 84.944 sec/batch)\n",
      "2017-06-19 09:43:53.596309: step 3 batch 170, loss = 15.30 (0.2 examples/sec; 90.228 sec/batch)\n",
      "2017-06-19 09:43:58.903843: step 3 batch 180, loss = 11.70 (0.2 examples/sec; 95.535 sec/batch)\n",
      "2017-06-19 09:44:04.164952: step 3 batch 190, loss = 13.15 (0.2 examples/sec; 100.797 sec/batch)\n",
      "2017-06-19 09:44:09.455731: step 3 batch 200, loss = 12.86 (0.2 examples/sec; 106.087 sec/batch)\n",
      "2017-06-19 09:44:14.793863: step 3 batch 210, loss = 14.35 (0.1 examples/sec; 111.426 sec/batch)\n",
      "2017-06-19 09:44:20.390988: step 3 batch 220, loss = 18.36 (0.1 examples/sec; 117.023 sec/batch)\n",
      "2017-06-19 09:44:25.716214: step 3 batch 230, loss = 19.80 (0.1 examples/sec; 122.348 sec/batch)\n",
      "2017-06-19 09:44:31.055950: step 3 batch 240, loss = 12.62 (0.1 examples/sec; 127.688 sec/batch)\n",
      "2017-06-19 09:44:36.354427: step 3 batch 250, loss = 9.99 (0.1 examples/sec; 132.986 sec/batch)\n",
      "2017-06-19 09:44:41.727980: step 3 batch 260, loss = 16.37 (0.1 examples/sec; 138.360 sec/batch)\n",
      "2017-06-19 09:44:47.062477: step 3 batch 270, loss = 9.85 (0.1 examples/sec; 143.694 sec/batch)\n",
      "2017-06-19 09:44:52.244503: step 3 batch 280, loss = 12.23 (0.1 examples/sec; 148.876 sec/batch)\n",
      "2017-06-19 09:44:57.581614: step 3 batch 290, loss = 13.38 (0.1 examples/sec; 154.213 sec/batch)\n",
      "2017-06-19 09:45:02.952058: step 3 batch 300, loss = 14.91 (0.1 examples/sec; 159.584 sec/batch)\n",
      "2017-06-19 09:45:08.268754: step 3 batch 310, loss = 13.92 (0.1 examples/sec; 164.900 sec/batch)\n",
      "2017-06-19 09:45:13.612694: step 3 batch 320, loss = 13.28 (0.1 examples/sec; 170.244 sec/batch)\n",
      "2017-06-19 09:45:18.897346: step 3 batch 330, loss = 14.91 (0.1 examples/sec; 175.529 sec/batch)\n",
      "2017-06-19 09:45:24.356962: step 3 batch 340, loss = 15.74 (0.1 examples/sec; 180.989 sec/batch)\n",
      "2017-06-19 09:45:29.616362: step 3 batch 350, loss = 14.41 (0.1 examples/sec; 186.248 sec/batch)\n",
      "2017-06-19 09:45:34.860297: step 3 batch 360, loss = 15.67 (0.1 examples/sec; 191.492 sec/batch)\n",
      "2017-06-19 09:45:40.188021: step 3 batch 370, loss = 14.64 (0.1 examples/sec; 196.820 sec/batch)\n",
      "2017-06-19 09:45:45.684911: step 3 batch 380, loss = 13.19 (0.1 examples/sec; 202.317 sec/batch)\n",
      "2017-06-19 09:45:50.972870: step 3 batch 390, loss = 11.45 (0.1 examples/sec; 207.605 sec/batch)\n",
      "2017-06-19 09:45:56.229359: step 3 batch 400, loss = 16.34 (0.1 examples/sec; 212.861 sec/batch)\n",
      "2017-06-19 09:46:01.460278: step 3 batch 410, loss = 12.21 (0.1 examples/sec; 218.092 sec/batch)\n",
      "2017-06-19 09:46:06.828254: step 3 batch 420, loss = 16.04 (0.1 examples/sec; 223.460 sec/batch)\n",
      "2017-06-19 09:46:12.069883: step 3 batch 430, loss = 10.04 (0.1 examples/sec; 228.702 sec/batch)\n",
      "2017-06-19 09:46:17.508342: step 3 batch 440, loss = 11.02 (0.1 examples/sec; 234.140 sec/batch)\n",
      "2017-06-19 09:46:22.713243: step 3 batch 450, loss = 11.53 (0.1 examples/sec; 239.345 sec/batch)\n",
      "2017-06-19 09:46:28.062420: step 3 batch 460, loss = 10.30 (0.1 examples/sec; 244.694 sec/batch)\n",
      "2017-06-19 09:46:33.272283: step 3 batch 470, loss = 9.47 (0.1 examples/sec; 249.904 sec/batch)\n",
      "2017-06-19 09:46:38.566054: step 3 batch 480, loss = 14.42 (0.1 examples/sec; 255.198 sec/batch)\n",
      "2017-06-19 09:46:43.855562: step 3 batch 490, loss = 10.01 (0.1 examples/sec; 260.487 sec/batch)\n",
      "2017-06-19 09:46:49.076462: step 3 batch 500, loss = 15.47 (0.1 examples/sec; 265.708 sec/batch)\n",
      "2017-06-19 09:46:54.513890: step 3 batch 510, loss = 18.14 (0.1 examples/sec; 271.146 sec/batch)\n",
      "2017-06-19 09:46:59.904237: step 3 batch 520, loss = 10.89 (0.1 examples/sec; 276.536 sec/batch)\n",
      "2017-06-19 09:47:05.131210: step 3 batch 530, loss = 12.52 (0.1 examples/sec; 281.763 sec/batch)\n",
      "2017-06-19 09:47:10.443138: step 3 batch 540, loss = 16.81 (0.1 examples/sec; 287.075 sec/batch)\n",
      "2017-06-19 09:47:15.895446: step 3 batch 550, loss = 14.48 (0.1 examples/sec; 292.527 sec/batch)\n",
      "2017-06-19 09:47:21.089256: step 3 batch 560, loss = 16.02 (0.1 examples/sec; 297.721 sec/batch)\n",
      "2017-06-19 09:47:26.395993: step 3 batch 570, loss = 14.76 (0.1 examples/sec; 303.028 sec/batch)\n",
      "2017-06-19 09:47:31.623734: step 3 batch 580, loss = 9.68 (0.1 examples/sec; 308.255 sec/batch)\n",
      "2017-06-19 09:47:36.727253: step 3 batch 590, loss = 14.42 (0.1 examples/sec; 313.359 sec/batch)\n",
      "2017-06-19 09:47:41.954222: step 3 batch 600, loss = 10.89 (0.1 examples/sec; 318.586 sec/batch)\n",
      "2017-06-19 09:47:47.393663: step 3 batch 610, loss = 10.02 (0.0 examples/sec; 324.025 sec/batch)\n",
      "2017-06-19 09:47:50.520069: step 4 batch 0, loss = 10.92 (31.2 examples/sec; 0.512 sec/batch)\n",
      "2017-06-19 09:47:56.009066: step 4 batch 10, loss = 12.05 (2.7 examples/sec; 6.001 sec/batch)\n",
      "2017-06-19 09:48:01.218462: step 4 batch 20, loss = 12.01 (1.4 examples/sec; 11.210 sec/batch)\n",
      "2017-06-19 09:48:06.589510: step 4 batch 30, loss = 10.76 (1.0 examples/sec; 16.581 sec/batch)\n",
      "2017-06-19 09:48:12.088360: step 4 batch 40, loss = 16.09 (0.7 examples/sec; 22.080 sec/batch)\n",
      "2017-06-19 09:48:17.280155: step 4 batch 50, loss = 12.26 (0.6 examples/sec; 27.272 sec/batch)\n",
      "2017-06-19 09:48:22.488848: step 4 batch 60, loss = 11.82 (0.5 examples/sec; 32.481 sec/batch)\n",
      "2017-06-19 09:48:27.988159: step 4 batch 70, loss = 12.96 (0.4 examples/sec; 37.980 sec/batch)\n",
      "2017-06-19 09:48:33.242897: step 4 batch 80, loss = 12.50 (0.4 examples/sec; 43.235 sec/batch)\n",
      "2017-06-19 09:48:38.484603: step 4 batch 90, loss = 13.30 (0.3 examples/sec; 48.477 sec/batch)\n",
      "2017-06-19 09:48:43.859697: step 4 batch 100, loss = 15.93 (0.3 examples/sec; 53.852 sec/batch)\n",
      "2017-06-19 09:48:49.142880: step 4 batch 110, loss = 12.90 (0.3 examples/sec; 59.135 sec/batch)\n",
      "2017-06-19 09:48:54.349880: step 4 batch 120, loss = 12.37 (0.2 examples/sec; 64.342 sec/batch)\n",
      "2017-06-19 09:48:59.544901: step 4 batch 130, loss = 12.12 (0.2 examples/sec; 69.537 sec/batch)\n",
      "2017-06-19 09:49:04.821885: step 4 batch 140, loss = 13.17 (0.2 examples/sec; 74.814 sec/batch)\n",
      "2017-06-19 09:49:09.934286: step 4 batch 150, loss = 14.05 (0.2 examples/sec; 79.926 sec/batch)\n",
      "2017-06-19 09:49:15.372191: step 4 batch 160, loss = 20.73 (0.2 examples/sec; 85.364 sec/batch)\n",
      "2017-06-19 09:49:20.878998: step 4 batch 170, loss = 17.83 (0.2 examples/sec; 90.871 sec/batch)\n",
      "2017-06-19 09:49:26.228677: step 4 batch 180, loss = 14.99 (0.2 examples/sec; 96.221 sec/batch)\n",
      "2017-06-19 09:49:31.528043: step 4 batch 190, loss = 11.54 (0.2 examples/sec; 101.520 sec/batch)\n",
      "2017-06-19 09:49:36.732554: step 4 batch 200, loss = 13.43 (0.1 examples/sec; 106.724 sec/batch)\n",
      "2017-06-19 09:49:41.999808: step 4 batch 210, loss = 14.02 (0.1 examples/sec; 111.992 sec/batch)\n",
      "2017-06-19 09:49:47.200230: step 4 batch 220, loss = 14.43 (0.1 examples/sec; 117.192 sec/batch)\n",
      "2017-06-19 09:49:52.384714: step 4 batch 230, loss = 20.66 (0.1 examples/sec; 122.377 sec/batch)\n",
      "2017-06-19 09:49:57.729118: step 4 batch 240, loss = 16.47 (0.1 examples/sec; 127.721 sec/batch)\n",
      "2017-06-19 09:50:03.046569: step 4 batch 250, loss = 12.67 (0.1 examples/sec; 133.039 sec/batch)\n",
      "2017-06-19 09:50:08.316255: step 4 batch 260, loss = 12.56 (0.1 examples/sec; 138.308 sec/batch)\n",
      "2017-06-19 09:50:13.727837: step 4 batch 270, loss = 10.58 (0.1 examples/sec; 143.720 sec/batch)\n",
      "2017-06-19 09:50:18.926194: step 4 batch 280, loss = 9.99 (0.1 examples/sec; 148.918 sec/batch)\n",
      "2017-06-19 09:50:24.402200: step 4 batch 290, loss = 13.47 (0.1 examples/sec; 154.394 sec/batch)\n",
      "2017-06-19 09:50:29.836128: step 4 batch 300, loss = 15.46 (0.1 examples/sec; 159.828 sec/batch)\n",
      "2017-06-19 09:50:35.134155: step 4 batch 310, loss = 13.43 (0.1 examples/sec; 165.126 sec/batch)\n",
      "2017-06-19 09:50:40.391775: step 4 batch 320, loss = 15.85 (0.1 examples/sec; 170.384 sec/batch)\n",
      "2017-06-19 09:50:45.655666: step 4 batch 330, loss = 11.85 (0.1 examples/sec; 175.648 sec/batch)\n",
      "2017-06-19 09:50:51.074391: step 4 batch 340, loss = 15.34 (0.1 examples/sec; 181.066 sec/batch)\n",
      "2017-06-19 09:50:56.413956: step 4 batch 350, loss = 13.02 (0.1 examples/sec; 186.406 sec/batch)\n",
      "2017-06-19 09:51:01.728602: step 4 batch 360, loss = 10.47 (0.1 examples/sec; 191.721 sec/batch)\n",
      "2017-06-19 09:51:06.927902: step 4 batch 370, loss = 12.34 (0.1 examples/sec; 196.920 sec/batch)\n",
      "2017-06-19 09:51:12.196757: step 4 batch 380, loss = 15.35 (0.1 examples/sec; 202.189 sec/batch)\n",
      "2017-06-19 09:51:17.338425: step 4 batch 390, loss = 11.83 (0.1 examples/sec; 207.330 sec/batch)\n",
      "2017-06-19 09:51:22.594678: step 4 batch 400, loss = 14.85 (0.1 examples/sec; 212.587 sec/batch)\n",
      "2017-06-19 09:51:27.874104: step 4 batch 410, loss = 10.36 (0.1 examples/sec; 217.866 sec/batch)\n",
      "2017-06-19 09:51:33.335939: step 4 batch 420, loss = 11.37 (0.1 examples/sec; 223.328 sec/batch)\n",
      "2017-06-19 09:51:38.673921: step 4 batch 430, loss = 12.80 (0.1 examples/sec; 228.666 sec/batch)\n",
      "2017-06-19 09:51:43.941771: step 4 batch 440, loss = 13.66 (0.1 examples/sec; 233.934 sec/batch)\n",
      "2017-06-19 09:51:49.220565: step 4 batch 450, loss = 13.55 (0.1 examples/sec; 239.213 sec/batch)\n",
      "2017-06-19 09:51:54.446145: step 4 batch 460, loss = 13.11 (0.1 examples/sec; 244.438 sec/batch)\n",
      "2017-06-19 09:51:59.796561: step 4 batch 470, loss = 11.62 (0.1 examples/sec; 249.789 sec/batch)\n",
      "2017-06-19 09:52:05.138865: step 4 batch 480, loss = 12.56 (0.1 examples/sec; 255.131 sec/batch)\n",
      "2017-06-19 09:52:10.479965: step 4 batch 490, loss = 7.15 (0.1 examples/sec; 260.472 sec/batch)\n",
      "2017-06-19 09:52:15.762170: step 4 batch 500, loss = 12.75 (0.1 examples/sec; 265.754 sec/batch)\n",
      "2017-06-19 09:52:20.950430: step 4 batch 510, loss = 13.69 (0.1 examples/sec; 270.942 sec/batch)\n",
      "2017-06-19 09:52:26.179684: step 4 batch 520, loss = 10.21 (0.1 examples/sec; 276.172 sec/batch)\n",
      "2017-06-19 09:52:31.494795: step 4 batch 530, loss = 18.15 (0.1 examples/sec; 281.487 sec/batch)\n",
      "2017-06-19 09:52:36.869705: step 4 batch 540, loss = 14.08 (0.1 examples/sec; 286.862 sec/batch)\n",
      "2017-06-19 09:52:42.148505: step 4 batch 550, loss = 13.08 (0.1 examples/sec; 292.140 sec/batch)\n",
      "2017-06-19 09:52:47.453849: step 4 batch 560, loss = 10.39 (0.1 examples/sec; 297.446 sec/batch)\n",
      "2017-06-19 09:52:52.756789: step 4 batch 570, loss = 10.84 (0.1 examples/sec; 302.749 sec/batch)\n",
      "2017-06-19 09:52:57.948485: step 4 batch 580, loss = 12.68 (0.1 examples/sec; 307.940 sec/batch)\n",
      "2017-06-19 09:53:03.350659: step 4 batch 590, loss = 13.48 (0.1 examples/sec; 313.343 sec/batch)\n",
      "2017-06-19 09:53:08.706848: step 4 batch 600, loss = 16.68 (0.1 examples/sec; 318.699 sec/batch)\n",
      "2017-06-19 09:53:14.042653: step 4 batch 610, loss = 14.74 (0.0 examples/sec; 324.035 sec/batch)\n",
      "2017-06-19 09:53:17.164475: step 5 batch 0, loss = 15.68 (29.4 examples/sec; 0.544 sec/batch)\n",
      "2017-06-19 09:53:22.538456: step 5 batch 10, loss = 12.85 (2.7 examples/sec; 5.918 sec/batch)\n",
      "2017-06-19 09:53:27.830900: step 5 batch 20, loss = 9.39 (1.4 examples/sec; 11.210 sec/batch)\n",
      "2017-06-19 09:53:32.988953: step 5 batch 30, loss = 10.23 (1.0 examples/sec; 16.369 sec/batch)\n",
      "2017-06-19 09:53:38.265615: step 5 batch 40, loss = 15.91 (0.7 examples/sec; 21.645 sec/batch)\n",
      "2017-06-19 09:53:43.541956: step 5 batch 50, loss = 13.20 (0.6 examples/sec; 26.922 sec/batch)\n",
      "2017-06-19 09:53:48.944936: step 5 batch 60, loss = 10.72 (0.5 examples/sec; 32.325 sec/batch)\n",
      "2017-06-19 09:53:54.196355: step 5 batch 70, loss = 15.58 (0.4 examples/sec; 37.576 sec/batch)\n",
      "2017-06-19 09:53:59.626553: step 5 batch 80, loss = 9.91 (0.4 examples/sec; 43.006 sec/batch)\n",
      "2017-06-19 09:54:05.113799: step 5 batch 90, loss = 20.73 (0.3 examples/sec; 48.493 sec/batch)\n",
      "2017-06-19 09:54:10.392069: step 5 batch 100, loss = 10.64 (0.3 examples/sec; 53.772 sec/batch)\n",
      "2017-06-19 09:54:15.746488: step 5 batch 110, loss = 13.47 (0.3 examples/sec; 59.126 sec/batch)\n",
      "2017-06-19 09:54:20.971854: step 5 batch 120, loss = 14.46 (0.2 examples/sec; 64.351 sec/batch)\n",
      "2017-06-19 09:54:26.117003: step 5 batch 130, loss = 14.53 (0.2 examples/sec; 69.497 sec/batch)\n",
      "2017-06-19 09:54:31.392181: step 5 batch 140, loss = 13.21 (0.2 examples/sec; 74.772 sec/batch)\n",
      "2017-06-19 09:54:36.740732: step 5 batch 150, loss = 13.57 (0.2 examples/sec; 80.120 sec/batch)\n",
      "2017-06-19 09:54:41.912421: step 5 batch 160, loss = 12.38 (0.2 examples/sec; 85.292 sec/batch)\n",
      "2017-06-19 09:54:47.162981: step 5 batch 170, loss = 14.75 (0.2 examples/sec; 90.543 sec/batch)\n",
      "2017-06-19 09:54:52.455058: step 5 batch 180, loss = 13.33 (0.2 examples/sec; 95.835 sec/batch)\n",
      "2017-06-19 09:54:57.904579: step 5 batch 190, loss = 14.54 (0.2 examples/sec; 101.284 sec/batch)\n",
      "2017-06-19 09:55:03.145340: step 5 batch 200, loss = 13.48 (0.2 examples/sec; 106.525 sec/batch)\n",
      "2017-06-19 09:55:08.403247: step 5 batch 210, loss = 15.65 (0.1 examples/sec; 111.783 sec/batch)\n",
      "2017-06-19 09:55:13.711341: step 5 batch 220, loss = 11.75 (0.1 examples/sec; 117.091 sec/batch)\n",
      "2017-06-19 09:55:19.119504: step 5 batch 230, loss = 13.76 (0.1 examples/sec; 122.499 sec/batch)\n",
      "2017-06-19 09:55:24.559118: step 5 batch 240, loss = 11.84 (0.1 examples/sec; 127.939 sec/batch)\n",
      "2017-06-19 09:55:29.691046: step 5 batch 250, loss = 12.18 (0.1 examples/sec; 133.071 sec/batch)\n",
      "2017-06-19 09:55:35.046886: step 5 batch 260, loss = 13.66 (0.1 examples/sec; 138.426 sec/batch)\n",
      "2017-06-19 09:55:40.532455: step 5 batch 270, loss = 13.56 (0.1 examples/sec; 143.912 sec/batch)\n",
      "2017-06-19 09:55:45.855958: step 5 batch 280, loss = 15.80 (0.1 examples/sec; 149.236 sec/batch)\n",
      "2017-06-19 09:55:51.037502: step 5 batch 290, loss = 12.67 (0.1 examples/sec; 154.417 sec/batch)\n",
      "2017-06-19 09:55:56.325865: step 5 batch 300, loss = 13.57 (0.1 examples/sec; 159.705 sec/batch)\n",
      "2017-06-19 09:56:01.676135: step 5 batch 310, loss = 14.90 (0.1 examples/sec; 165.056 sec/batch)\n",
      "2017-06-19 09:56:07.053757: step 5 batch 320, loss = 11.17 (0.1 examples/sec; 170.433 sec/batch)\n",
      "2017-06-19 09:56:12.491900: step 5 batch 330, loss = 11.65 (0.1 examples/sec; 175.871 sec/batch)\n",
      "2017-06-19 09:56:17.750862: step 5 batch 340, loss = 9.65 (0.1 examples/sec; 181.130 sec/batch)\n",
      "2017-06-19 09:56:22.998470: step 5 batch 350, loss = 14.79 (0.1 examples/sec; 186.378 sec/batch)\n",
      "2017-06-19 09:56:28.298131: step 5 batch 360, loss = 11.03 (0.1 examples/sec; 191.678 sec/batch)\n",
      "2017-06-19 09:56:33.620375: step 5 batch 370, loss = 9.59 (0.1 examples/sec; 197.000 sec/batch)\n",
      "2017-06-19 09:56:38.842792: step 5 batch 380, loss = 10.95 (0.1 examples/sec; 202.222 sec/batch)\n",
      "2017-06-19 09:56:44.035587: step 5 batch 390, loss = 14.06 (0.1 examples/sec; 207.415 sec/batch)\n",
      "2017-06-19 09:56:49.430810: step 5 batch 400, loss = 11.16 (0.1 examples/sec; 212.810 sec/batch)\n",
      "2017-06-19 09:56:54.814286: step 5 batch 410, loss = 12.99 (0.1 examples/sec; 218.194 sec/batch)\n",
      "2017-06-19 09:56:59.918628: step 5 batch 420, loss = 9.86 (0.1 examples/sec; 223.298 sec/batch)\n",
      "2017-06-19 09:57:05.086046: step 5 batch 430, loss = 14.03 (0.1 examples/sec; 228.466 sec/batch)\n",
      "2017-06-19 09:57:10.544951: step 5 batch 440, loss = 12.75 (0.1 examples/sec; 233.925 sec/batch)\n",
      "2017-06-19 09:57:15.757818: step 5 batch 450, loss = 15.35 (0.1 examples/sec; 239.137 sec/batch)\n",
      "2017-06-19 09:57:21.113695: step 5 batch 460, loss = 13.43 (0.1 examples/sec; 244.493 sec/batch)\n",
      "2017-06-19 09:57:26.381829: step 5 batch 470, loss = 13.67 (0.1 examples/sec; 249.761 sec/batch)\n",
      "2017-06-19 09:57:31.669586: step 5 batch 480, loss = 13.12 (0.1 examples/sec; 255.049 sec/batch)\n",
      "2017-06-19 09:57:36.900679: step 5 batch 490, loss = 11.71 (0.1 examples/sec; 260.280 sec/batch)\n",
      "2017-06-19 09:57:42.166533: step 5 batch 500, loss = 12.95 (0.1 examples/sec; 265.546 sec/batch)\n",
      "2017-06-19 09:57:47.505795: step 5 batch 510, loss = 17.81 (0.1 examples/sec; 270.885 sec/batch)\n",
      "2017-06-19 09:57:52.893464: step 5 batch 520, loss = 11.78 (0.1 examples/sec; 276.273 sec/batch)\n",
      "2017-06-19 09:57:58.145240: step 5 batch 530, loss = 11.96 (0.1 examples/sec; 281.525 sec/batch)\n",
      "2017-06-19 09:58:03.345927: step 5 batch 540, loss = 12.82 (0.1 examples/sec; 286.725 sec/batch)\n",
      "2017-06-19 09:58:08.712804: step 5 batch 550, loss = 18.66 (0.1 examples/sec; 292.092 sec/batch)\n",
      "2017-06-19 09:58:14.072443: step 5 batch 560, loss = 18.84 (0.1 examples/sec; 297.452 sec/batch)\n",
      "2017-06-19 09:58:19.210876: step 5 batch 570, loss = 11.50 (0.1 examples/sec; 302.590 sec/batch)\n",
      "2017-06-19 09:58:24.436198: step 5 batch 580, loss = 11.51 (0.1 examples/sec; 307.816 sec/batch)\n",
      "2017-06-19 09:58:29.587393: step 5 batch 590, loss = 9.98 (0.1 examples/sec; 312.967 sec/batch)\n",
      "2017-06-19 09:58:34.913833: step 5 batch 600, loss = 15.51 (0.1 examples/sec; 318.293 sec/batch)\n",
      "2017-06-19 09:58:40.219284: step 5 batch 610, loss = 12.29 (0.0 examples/sec; 323.599 sec/batch)\n",
      "2017-06-19 09:58:43.365128: step 6 batch 0, loss = 10.54 (30.4 examples/sec; 0.526 sec/batch)\n",
      "2017-06-19 09:58:48.529306: step 6 batch 10, loss = 14.23 (2.8 examples/sec; 5.690 sec/batch)\n",
      "2017-06-19 09:58:53.687575: step 6 batch 20, loss = 14.62 (1.5 examples/sec; 10.848 sec/batch)\n",
      "2017-06-19 09:58:59.252908: step 6 batch 30, loss = 13.40 (1.0 examples/sec; 16.413 sec/batch)\n",
      "2017-06-19 09:59:04.575547: step 6 batch 40, loss = 10.23 (0.7 examples/sec; 21.736 sec/batch)\n",
      "2017-06-19 09:59:09.990934: step 6 batch 50, loss = 10.33 (0.6 examples/sec; 27.151 sec/batch)\n",
      "2017-06-19 09:59:15.420934: step 6 batch 60, loss = 13.46 (0.5 examples/sec; 32.581 sec/batch)\n",
      "2017-06-19 09:59:20.946929: step 6 batch 70, loss = 11.48 (0.4 examples/sec; 38.107 sec/batch)\n",
      "2017-06-19 09:59:26.153458: step 6 batch 80, loss = 11.17 (0.4 examples/sec; 43.314 sec/batch)\n",
      "2017-06-19 09:59:31.436631: step 6 batch 90, loss = 7.75 (0.3 examples/sec; 48.597 sec/batch)\n",
      "2017-06-19 09:59:36.882480: step 6 batch 100, loss = 14.08 (0.3 examples/sec; 54.043 sec/batch)\n",
      "2017-06-19 09:59:42.080545: step 6 batch 110, loss = 15.55 (0.3 examples/sec; 59.241 sec/batch)\n",
      "2017-06-19 09:59:47.314071: step 6 batch 120, loss = 12.46 (0.2 examples/sec; 64.475 sec/batch)\n",
      "2017-06-19 09:59:52.676623: step 6 batch 130, loss = 12.41 (0.2 examples/sec; 69.837 sec/batch)\n",
      "2017-06-19 09:59:57.897745: step 6 batch 140, loss = 13.81 (0.2 examples/sec; 75.058 sec/batch)\n",
      "2017-06-19 10:00:03.375256: step 6 batch 150, loss = 15.61 (0.2 examples/sec; 80.536 sec/batch)\n",
      "2017-06-19 10:00:08.524905: step 6 batch 160, loss = 14.24 (0.2 examples/sec; 85.685 sec/batch)\n",
      "2017-06-19 10:00:13.793826: step 6 batch 170, loss = 16.21 (0.2 examples/sec; 90.954 sec/batch)\n",
      "2017-06-19 10:00:19.299033: step 6 batch 180, loss = 15.96 (0.2 examples/sec; 96.460 sec/batch)\n",
      "2017-06-19 10:00:24.627052: step 6 batch 190, loss = 8.23 (0.2 examples/sec; 101.788 sec/batch)\n",
      "2017-06-19 10:00:30.064209: step 6 batch 200, loss = 12.20 (0.1 examples/sec; 107.225 sec/batch)\n",
      "2017-06-19 10:00:35.368610: step 6 batch 210, loss = 16.04 (0.1 examples/sec; 112.529 sec/batch)\n",
      "2017-06-19 10:00:40.531407: step 6 batch 220, loss = 10.47 (0.1 examples/sec; 117.692 sec/batch)\n",
      "2017-06-19 10:00:45.814183: step 6 batch 230, loss = 12.34 (0.1 examples/sec; 122.975 sec/batch)\n",
      "2017-06-19 10:00:51.099380: step 6 batch 240, loss = 14.61 (0.1 examples/sec; 128.260 sec/batch)\n",
      "2017-06-19 10:00:56.477112: step 6 batch 250, loss = 11.56 (0.1 examples/sec; 133.638 sec/batch)\n",
      "2017-06-19 10:01:01.852009: step 6 batch 260, loss = 13.49 (0.1 examples/sec; 139.012 sec/batch)\n",
      "2017-06-19 10:01:07.111159: step 6 batch 270, loss = 10.24 (0.1 examples/sec; 144.272 sec/batch)\n",
      "2017-06-19 10:01:12.415721: step 6 batch 280, loss = 15.59 (0.1 examples/sec; 149.576 sec/batch)\n",
      "2017-06-19 10:01:17.726896: step 6 batch 290, loss = 10.36 (0.1 examples/sec; 154.887 sec/batch)\n",
      "2017-06-19 10:01:23.078009: step 6 batch 300, loss = 10.19 (0.1 examples/sec; 160.238 sec/batch)\n",
      "2017-06-19 10:01:28.381395: step 6 batch 310, loss = 13.07 (0.1 examples/sec; 165.542 sec/batch)\n",
      "2017-06-19 10:01:33.725014: step 6 batch 320, loss = 12.48 (0.1 examples/sec; 170.885 sec/batch)\n",
      "2017-06-19 10:01:38.934352: step 6 batch 330, loss = 12.39 (0.1 examples/sec; 176.095 sec/batch)\n",
      "2017-06-19 10:01:44.365017: step 6 batch 340, loss = 18.03 (0.1 examples/sec; 181.525 sec/batch)\n",
      "2017-06-19 10:01:49.697099: step 6 batch 350, loss = 11.21 (0.1 examples/sec; 186.858 sec/batch)\n",
      "2017-06-19 10:01:54.954141: step 6 batch 360, loss = 15.06 (0.1 examples/sec; 192.115 sec/batch)\n",
      "2017-06-19 10:02:00.183700: step 6 batch 370, loss = 10.98 (0.1 examples/sec; 197.344 sec/batch)\n",
      "2017-06-19 10:02:05.455034: step 6 batch 380, loss = 8.55 (0.1 examples/sec; 202.616 sec/batch)\n",
      "2017-06-19 10:02:10.638597: step 6 batch 390, loss = 8.83 (0.1 examples/sec; 207.799 sec/batch)\n",
      "2017-06-19 10:02:15.873793: step 6 batch 400, loss = 16.25 (0.1 examples/sec; 213.034 sec/batch)\n",
      "2017-06-19 10:02:21.067842: step 6 batch 410, loss = 16.96 (0.1 examples/sec; 218.228 sec/batch)\n",
      "2017-06-19 10:02:26.560409: step 6 batch 420, loss = 14.16 (0.1 examples/sec; 223.721 sec/batch)\n",
      "2017-06-19 10:02:32.002068: step 6 batch 430, loss = 12.82 (0.1 examples/sec; 229.163 sec/batch)\n",
      "2017-06-19 10:02:37.176970: step 6 batch 440, loss = 7.54 (0.1 examples/sec; 234.337 sec/batch)\n",
      "2017-06-19 10:02:42.522819: step 6 batch 450, loss = 14.91 (0.1 examples/sec; 239.683 sec/batch)\n",
      "2017-06-19 10:02:48.065693: step 6 batch 460, loss = 10.40 (0.1 examples/sec; 245.226 sec/batch)\n",
      "2017-06-19 10:02:53.264836: step 6 batch 470, loss = 11.62 (0.1 examples/sec; 250.425 sec/batch)\n",
      "2017-06-19 10:02:58.689979: step 6 batch 480, loss = 16.96 (0.1 examples/sec; 255.850 sec/batch)\n",
      "2017-06-19 10:03:04.224918: step 6 batch 490, loss = 15.20 (0.1 examples/sec; 261.385 sec/batch)\n",
      "2017-06-19 10:03:09.548742: step 6 batch 500, loss = 17.18 (0.1 examples/sec; 266.709 sec/batch)\n",
      "2017-06-19 10:03:14.850177: step 6 batch 510, loss = 11.95 (0.1 examples/sec; 272.011 sec/batch)\n",
      "2017-06-19 10:03:20.119331: step 6 batch 520, loss = 13.18 (0.1 examples/sec; 277.280 sec/batch)\n",
      "2017-06-19 10:03:25.288814: step 6 batch 530, loss = 11.95 (0.1 examples/sec; 282.449 sec/batch)\n",
      "2017-06-19 10:03:30.584442: step 6 batch 540, loss = 11.36 (0.1 examples/sec; 287.745 sec/batch)\n",
      "2017-06-19 10:03:36.150448: step 6 batch 550, loss = 13.61 (0.1 examples/sec; 293.311 sec/batch)\n",
      "2017-06-19 10:03:41.349274: step 6 batch 560, loss = 9.23 (0.1 examples/sec; 298.510 sec/batch)\n",
      "2017-06-19 10:03:46.725046: step 6 batch 570, loss = 11.45 (0.1 examples/sec; 303.886 sec/batch)\n",
      "2017-06-19 10:03:52.255105: step 6 batch 580, loss = 12.81 (0.1 examples/sec; 309.416 sec/batch)\n",
      "2017-06-19 10:03:57.839121: step 6 batch 590, loss = 9.26 (0.1 examples/sec; 315.000 sec/batch)\n",
      "2017-06-19 10:04:03.334948: step 6 batch 600, loss = 13.88 (0.0 examples/sec; 320.495 sec/batch)\n",
      "2017-06-19 10:04:08.718565: step 6 batch 610, loss = 8.97 (0.0 examples/sec; 325.879 sec/batch)\n",
      "2017-06-19 10:04:11.869057: step 7 batch 0, loss = 6.92 (31.0 examples/sec; 0.516 sec/batch)\n",
      "2017-06-19 10:04:17.399595: step 7 batch 10, loss = 12.20 (2.6 examples/sec; 6.046 sec/batch)\n",
      "2017-06-19 10:04:22.651991: step 7 batch 20, loss = 11.21 (1.4 examples/sec; 11.299 sec/batch)\n",
      "2017-06-19 10:04:27.952331: step 7 batch 30, loss = 7.78 (1.0 examples/sec; 16.599 sec/batch)\n",
      "2017-06-19 10:04:33.275621: step 7 batch 40, loss = 11.13 (0.7 examples/sec; 21.922 sec/batch)\n",
      "2017-06-19 10:04:38.665409: step 7 batch 50, loss = 13.79 (0.6 examples/sec; 27.312 sec/batch)\n",
      "2017-06-19 10:04:44.056473: step 7 batch 60, loss = 13.84 (0.5 examples/sec; 32.703 sec/batch)\n",
      "2017-06-19 10:04:49.270207: step 7 batch 70, loss = 10.09 (0.4 examples/sec; 37.917 sec/batch)\n",
      "2017-06-19 10:04:54.863724: step 7 batch 80, loss = 20.16 (0.4 examples/sec; 43.510 sec/batch)\n",
      "2017-06-19 10:05:00.317339: step 7 batch 90, loss = 13.36 (0.3 examples/sec; 48.964 sec/batch)\n",
      "2017-06-19 10:05:05.810479: step 7 batch 100, loss = 14.39 (0.3 examples/sec; 54.457 sec/batch)\n",
      "2017-06-19 10:05:11.117245: step 7 batch 110, loss = 14.52 (0.3 examples/sec; 59.764 sec/batch)\n",
      "2017-06-19 10:05:16.518886: step 7 batch 120, loss = 12.83 (0.2 examples/sec; 65.166 sec/batch)\n",
      "2017-06-19 10:05:22.075142: step 7 batch 130, loss = 11.63 (0.2 examples/sec; 70.722 sec/batch)\n",
      "2017-06-19 10:05:27.418111: step 7 batch 140, loss = 13.39 (0.2 examples/sec; 76.065 sec/batch)\n",
      "2017-06-19 10:05:32.836820: step 7 batch 150, loss = 16.18 (0.2 examples/sec; 81.484 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "for step in range(20):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    random.shuffle(record_list)\n",
    "    for i_batch in range(num_batch_per_epoch):\n",
    "        if i_batch + 1 == num_batch_per_epoch:\n",
    "            break\n",
    "#             l = record_list[i_batch * batch_size:]\n",
    "        else:\n",
    "            l = record_list[i_batch * batch_size:i_batch * batch_size + batch_size]\n",
    "        np_images = []\n",
    "        np_labels = []\n",
    "        np_objects_num = []\n",
    "        for item in l:\n",
    "            image, label, object_num = record_process(item)\n",
    "            image = np.array(image)\n",
    "            image = image.astype(np.float32)\n",
    "            np_images.append(image)\n",
    "            np_labels.append(label)\n",
    "            np_objects_num.append(object_num)\n",
    "        np_images = np.asarray(np_images, dtype=np.float32)\n",
    "        np_images = np_images/255 * 2 - 1\n",
    "        np_labels = np.asarray(np_labels, dtype=np.float32)\n",
    "        np_objects_num = np.asarray(np_objects_num, dtype=np.int32)\n",
    "        _, loss_value = sess.run([opt, total_loss], feed_dict={images: np_images, labels: np_labels, objects_num: np_objects_num})\n",
    "        duration = time.time() - start_time\n",
    "        if i_batch % 10 == 0:\n",
    "            num_examples_per_step = batch_size\n",
    "            examples_per_sec = num_examples_per_step / duration\n",
    "            sec_per_batch = float(duration)\n",
    "\n",
    "            format_str = ('%s: step %d batch %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                          'sec/batch)')\n",
    "            print (format_str % (datetime.now(), step, i_batch, loss_value,\n",
    "                                 examples_per_sec, sec_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolo2d_model/model.ckpt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saver = tf.train.Saver()\n",
    "saveDir = 'yolo2d_model'\n",
    "saver.save(sess, saveDir + '/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70.958498001098633, 84.109132766723633, 379.56708335876465, 363.02048301696777, 'cat')\n"
     ]
    }
   ],
   "source": [
    "##test\n",
    "def process_predicts(predicts):\n",
    "    p_classes = predicts[0, :, :, 0:20]\n",
    "    C = predicts[0, :, :, 20:22]\n",
    "    coordinate = predicts[0, :, :, 22:]\n",
    "\n",
    "    p_classes = np.reshape(p_classes, (7, 7, 1, 20))\n",
    "    C = np.reshape(C, (7, 7, 2, 1))\n",
    "\n",
    "    P = C * p_classes\n",
    "\n",
    "    #print P[5,1, 0, :]\n",
    "\n",
    "    index = np.argmax(P)\n",
    "\n",
    "    index = np.unravel_index(index, P.shape)\n",
    "\n",
    "    class_num = index[3]\n",
    "\n",
    "    coordinate = np.reshape(coordinate, (7, 7, 2, 4))\n",
    "\n",
    "    max_coordinate = coordinate[index[0], index[1], index[2], :]\n",
    "\n",
    "    xcenter = max_coordinate[0]\n",
    "    ycenter = max_coordinate[1]\n",
    "    w = max_coordinate[2]\n",
    "    h = max_coordinate[3]\n",
    "\n",
    "    xcenter = (index[1] + xcenter) * (448/7.0)\n",
    "    ycenter = (index[0] + ycenter) * (448/7.0)\n",
    "\n",
    "    w = w * 448\n",
    "    h = h * 448\n",
    "\n",
    "    xmin = xcenter - w/2.0\n",
    "    ymin = ycenter - h/2.0\n",
    "\n",
    "    xmax = xmin + w\n",
    "    ymax = ymin + h\n",
    "\n",
    "    return xmin, ymin, xmax, ymax, class_num\n",
    "\n",
    "classes_name =  [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\",\"tvmonitor\"]\n",
    "\n",
    "np_images = []\n",
    "# np_img = cv2.imread('cat.jpg')\n",
    "np_img = Image.open('cat.jpg')\n",
    "# resized_img = cv2.resize(np_img, (height, width))\n",
    "resized_img = np_img.resize((height, width))\n",
    "# np_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
    "np_img = cvtColor(resized_img)\n",
    "np_img = np.array(np_img)\n",
    "np_img = np_img.astype(np.float32)\n",
    "np_images.append(np_img)\n",
    "np_images = np.asarray(np_images, dtype=np.float32)\n",
    "np_images = np_images/255 * 2 - 1\n",
    "np_predict = sess.run(predicts, feed_dict={images: np_images})\n",
    "\n",
    "xmin, ymin, xmax, ymax, class_num = process_predicts(np_predict)\n",
    "class_name = classes_name[class_num]\n",
    "print(xmin, ymin, xmax, ymax, class_name)\n",
    "# cv2.rectangle(resized_img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 0, 255))\n",
    "# cv2.putText(resized_img, class_name, (int(xmin), int(ymin)), 2, 1.5, (0, 0, 255))\n",
    "# cv2.imwrite('cat_out.jpg', resized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
