{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import cv2\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.1, dtype=tf.float32):\n",
    "    x = tf.cast(x, dtype=dtype)\n",
    "    bool_mask = (x > 0)\n",
    "    mask = tf.cast(bool_mask, dtype=dtype)\n",
    "    return 1.0 * mask * x + alpha * (1 - mask) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(scope, input, kernel_size, stride=1, pretrain=True, train=True):\n",
    "    \"\"\"convolutional layer\n",
    "\n",
    "    Args:\n",
    "      input: 4-D tensor [batch_size, height, width, depth]\n",
    "      scope: variable_scope name\n",
    "      kernel_size: [k_height, k_width, in_channel, out_channel]\n",
    "      stride: int32\n",
    "    Return:\n",
    "      output: 4-D tensor [batch_size, height/stride, width/stride, out_channels]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "#         kernel = _variable_with_weight_decay('weights', shape = kernel_size, stddev = 5e-2)\n",
    "#         var = self._variable_on_cpu(name, shape, tf.truncated_normal_initializer(stddev=stddev, dtype=tf.float32))\n",
    "        kernel = tf.get_variable('weights', kernel_size, initializer=tf.truncated_normal_initializer(stddev=5e-2, dtype=tf.float32), dtype=tf.float32)\n",
    "        conv = tf.nn.conv2d(input, kernel, [1, stride, stride, 1], padding='SAME')\n",
    "        biases = tf.get_variable('biases', kernel_size[3:], initializer = tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "#         biases = self._variable_on_cpu('biases', kernel_size[3:], tf.constant_initializer(0.0), pretrain, train)\n",
    "        conv_plus_biases = tf.nn.bias_add(conv, biases)\n",
    "        conv = leaky_relu(conv_plus_biases)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(input, kernel_size, stride):\n",
    "    \"\"\"max_pool layer\n",
    "\n",
    "    Args:\n",
    "      input: 4-D tensor [batch_zie, height, width, depth]\n",
    "      kernel_size: [k_height, k_width]\n",
    "      stride: int32\n",
    "    Return:\n",
    "      output: 4-D tensor [batch_size, height/stride, width/stride, depth]\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(input, ksize=[1, kernel_size[0], kernel_size[1], 1], strides=[1, stride, stride, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def local(scope, input, in_dimension, out_dimension, leaky=True):\n",
    "    \"\"\"Fully connection layer\n",
    "\n",
    "    Args:\n",
    "      scope: variable_scope name\n",
    "      input: [batch_size, ???]\n",
    "      out_dimension: int32\n",
    "    Return:\n",
    "      output: 2-D tensor [batch_size, out_dimension]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        reshape = tf.reshape(input, [tf.shape(input)[0], -1])\n",
    "\n",
    "#         weights = self._variable_with_weight_decay('weights', shape=[in_dimension, out_dimension],\n",
    "#                                                  stddev=0.04, wd=self.weight_decay, pretrain=pretrain, train=train)\n",
    "#         var = self._variable_on_cpu(name, shape,\n",
    "#       tf.truncated_normal_initializer(stddev=stddev, dtype=tf.float32), pretrain, train)\n",
    "        weights = tf.get_variable('weights', [in_dimension, out_dimension], initializer=tf.truncated_normal_initializer(stddev=0.04, dtype=tf.float32), dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "#         biases = self._variable_on_cpu('biases', [out_dimension], tf.constant_initializer(0.0), pretrain, train)\n",
    "        biases = tf.get_variable('biases', [out_dimension], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "        \n",
    "        local = tf.matmul(reshape, weights) + biases\n",
    "\n",
    "        if leaky:\n",
    "            local = leaky_relu(local)\n",
    "        else:\n",
    "            local = tf.identity(local, name=scope.name)\n",
    "\n",
    "    return local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cond1(num, object_num, loss, predict, label, nilboy):\n",
    "    \"\"\"\n",
    "    if num < object_num\n",
    "    \"\"\"\n",
    "    return num < object_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iou(boxes1, boxes2):\n",
    "    \"\"\"calculate ious\n",
    "    Args:\n",
    "      boxes1: 4-D tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4]  ====> (x_center, y_center, w, h)\n",
    "      boxes2: 1-D tensor [4] ===> (x_center, y_center, w, h)\n",
    "    Return:\n",
    "      iou: 3-D tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    \"\"\"\n",
    "    boxes1 = tf.stack([boxes1[:, :, :, 0] - boxes1[:, :, :, 2] / 2, boxes1[:, :, :, 1] - boxes1[:, :, :, 3] / 2,\n",
    "                      boxes1[:, :, :, 0] + boxes1[:, :, :, 2] / 2, boxes1[:, :, :, 1] + boxes1[:, :, :, 3] / 2])\n",
    "    boxes1 = tf.transpose(boxes1, [1, 2, 3, 0])\n",
    "    boxes2 =  tf.stack([boxes2[0] - boxes2[2] / 2, boxes2[1] - boxes2[3] / 2,\n",
    "                      boxes2[0] + boxes2[2] / 2, boxes2[1] + boxes2[3] / 2])\n",
    "\n",
    "    #calculate the left up point\n",
    "    lu = tf.maximum(boxes1[:, :, :, 0:2], boxes2[0:2])\n",
    "    rd = tf.minimum(boxes1[:, :, :, 2:], boxes2[2:])\n",
    "\n",
    "    #intersection\n",
    "    intersection = rd - lu \n",
    "\n",
    "    inter_square = intersection[:, :, :, 0] * intersection[:, :, :, 1]\n",
    "\n",
    "    mask = tf.cast(intersection[:, :, :, 0] > 0, tf.float32) * tf.cast(intersection[:, :, :, 1] > 0, tf.float32)\n",
    "    \n",
    "    inter_square = mask * inter_square\n",
    "    \n",
    "    #calculate the boxs1 square and boxs2 square\n",
    "    square1 = (boxes1[:, :, :, 2] - boxes1[:, :, :, 0]) * (boxes1[:, :, :, 3] - boxes1[:, :, :, 1])\n",
    "    square2 = (boxes2[2] - boxes2[0]) * (boxes2[3] - boxes2[1])\n",
    "    \n",
    "    return inter_square/(square1 + square2 - inter_square + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def body1(num, object_num, loss, predict, labels, nilboy):\n",
    "    \"\"\"\n",
    "    calculate loss\n",
    "    Args:\n",
    "      predict: 3-D tensor [cell_size, cell_size, 5 * boxes_per_cell]\n",
    "      labels : [max_objects, 5]  (x_center, y_center, w, h, class)\n",
    "    \"\"\"\n",
    "    global image_size\n",
    "    global cell_size\n",
    "    \n",
    "    label = labels[num:num+1, :]\n",
    "    label = tf.reshape(label, [-1])\n",
    "\n",
    "    #calculate objects  tensor [CELL_SIZE, CELL_SIZE]\n",
    "    min_x = (label[0] - label[2] / 2) / (image_size / cell_size)\n",
    "    max_x = (label[0] + label[2] / 2) / (image_size / cell_size)\n",
    "\n",
    "    min_y = (label[1] - label[3] / 2) / (image_size / cell_size)\n",
    "    max_y = (label[1] + label[3] / 2) / (image_size / cell_size)\n",
    "\n",
    "    min_x = tf.floor(min_x)\n",
    "    min_y = tf.floor(min_y)\n",
    "\n",
    "    max_x = tf.ceil(max_x)\n",
    "    max_y = tf.ceil(max_y)\n",
    "\n",
    "    temp = tf.cast(tf.stack([max_y - min_y, max_x - min_x]), dtype=tf.int32)\n",
    "    objects = tf.ones(temp, tf.float32)\n",
    "\n",
    "    temp = tf.cast(tf.stack([min_y, cell_size - max_y, min_x, cell_size - max_x]), tf.int32)\n",
    "    temp = tf.reshape(temp, (2, 2))\n",
    "    objects = tf.pad(objects, temp, \"CONSTANT\")\n",
    "\n",
    "    #calculate objects  tensor [CELL_SIZE, CELL_SIZE]\n",
    "    #calculate responsible tensor [CELL_SIZE, CELL_SIZE]\n",
    "    center_x = label[0] / (image_size / cell_size)\n",
    "    center_x = tf.floor(center_x)\n",
    "\n",
    "    center_y = label[1] / (image_size / cell_size)\n",
    "    center_y = tf.floor(center_y)\n",
    "\n",
    "    response = tf.ones([1, 1], tf.float32)\n",
    "\n",
    "    temp = tf.cast(tf.stack([center_y, cell_size - center_y - 1, center_x, cell_size -center_x - 1]), tf.int32)\n",
    "    temp = tf.reshape(temp, (2, 2))\n",
    "    response = tf.pad(response, temp, \"CONSTANT\")\n",
    "    #objects = response\n",
    "\n",
    "    #calculate iou_predict_truth [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    predict_boxes = predict[:, :, num_classes + boxes_per_cell:]\n",
    "    \n",
    "\n",
    "    predict_boxes = tf.reshape(predict_boxes, [cell_size, cell_size, boxes_per_cell, 4])\n",
    "\n",
    "    predict_boxes = predict_boxes * [image_size / cell_size, image_size / cell_size, image_size, image_size]\n",
    "\n",
    "    base_boxes = np.zeros([cell_size, cell_size, 4])\n",
    "\n",
    "    for y in range(cell_size):\n",
    "        for x in range(cell_size):\n",
    "            #nilboy\n",
    "            base_boxes[y, x, :] = [image_size / cell_size * x, image_size / cell_size * y, 0, 0]\n",
    "    base_boxes = np.tile(np.resize(base_boxes, [cell_size, cell_size, 1, 4]), [1, 1, boxes_per_cell, 1])\n",
    "\n",
    "    predict_boxes = base_boxes + predict_boxes\n",
    "\n",
    "    iou_predict_truth = iou(predict_boxes, label[0:4])\n",
    "    #calculate C [cell_size, cell_size, boxes_per_cell]\n",
    "    C = iou_predict_truth * tf.reshape(response, [cell_size, cell_size, 1])\n",
    "\n",
    "    #calculate I tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    I = iou_predict_truth * tf.reshape(response, (cell_size, cell_size, 1))\n",
    "    \n",
    "    max_I = tf.reduce_max(I, 2, keep_dims=True)\n",
    "\n",
    "    I = tf.cast((I >= max_I), tf.float32) * tf.reshape(response, (cell_size, cell_size, 1))\n",
    "\n",
    "    #calculate no_I tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    no_I = tf.ones_like(I, dtype=tf.float32) - I \n",
    "\n",
    "\n",
    "    p_C = predict[:, :, num_classes:num_classes + boxes_per_cell]\n",
    "\n",
    "    #calculate truth x,y,sqrt_w,sqrt_h 0-D\n",
    "    x = label[0]\n",
    "    y = label[1]\n",
    "\n",
    "    sqrt_w = tf.sqrt(tf.abs(label[2]))\n",
    "    sqrt_h = tf.sqrt(tf.abs(label[3]))\n",
    "    #sqrt_w = tf.abs(label[2])\n",
    "    #sqrt_h = tf.abs(label[3])\n",
    "\n",
    "    #calculate predict p_x, p_y, p_sqrt_w, p_sqrt_h 3-D [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    p_x = predict_boxes[:, :, :, 0]\n",
    "    p_y = predict_boxes[:, :, :, 1]\n",
    "\n",
    "    #p_sqrt_w = tf.sqrt(tf.abs(predict_boxes[:, :, :, 2])) * ((tf.cast(predict_boxes[:, :, :, 2] > 0, tf.float32) * 2) - 1)\n",
    "    #p_sqrt_h = tf.sqrt(tf.abs(predict_boxes[:, :, :, 3])) * ((tf.cast(predict_boxes[:, :, :, 3] > 0, tf.float32) * 2) - 1)\n",
    "    #p_sqrt_w = tf.sqrt(tf.maximum(0.0, predict_boxes[:, :, :, 2]))\n",
    "    #p_sqrt_h = tf.sqrt(tf.maximum(0.0, predict_boxes[:, :, :, 3]))\n",
    "    #p_sqrt_w = predict_boxes[:, :, :, 2]\n",
    "    #p_sqrt_h = predict_boxes[:, :, :, 3]\n",
    "    p_sqrt_w = tf.sqrt(tf.minimum(image_size * 1.0, tf.maximum(0.0, predict_boxes[:, :, :, 2])))\n",
    "    p_sqrt_h = tf.sqrt(tf.minimum(image_size * 1.0, tf.maximum(0.0, predict_boxes[:, :, :, 3])))\n",
    "    #calculate truth p 1-D tensor [NUM_CLASSES]\n",
    "    P = tf.one_hot(tf.cast(label[4], tf.int32), num_classes, dtype=tf.float32)\n",
    "\n",
    "    #calculate predict p_P 3-D tensor [CELL_SIZE, CELL_SIZE, NUM_CLASSES]\n",
    "    p_P = predict[:, :, 0:num_classes]\n",
    "\n",
    "    #class_loss\n",
    "    class_loss = tf.nn.l2_loss(tf.reshape(objects, (cell_size, cell_size, 1)) * (p_P - P)) * class_scale\n",
    "    #class_loss = tf.nn.l2_loss(tf.reshape(response, (cell_size, cell_size, 1)) * (p_P - P)) * class_scale\n",
    "\n",
    "    #object_loss\n",
    "    object_loss = tf.nn.l2_loss(I * (p_C - C)) * object_scale\n",
    "    #object_loss = tf.nn.l2_loss(I * (p_C - (C + 1.0)/2.0)) * object_scale\n",
    "\n",
    "    #noobject_loss\n",
    "    #noobject_loss = tf.nn.l2_loss(no_I * (p_C - C)) * noobject_scale\n",
    "    noobject_loss = tf.nn.l2_loss(no_I * (p_C)) * noobject_scale\n",
    "\n",
    "    #coord_loss\n",
    "    coord_loss = (tf.nn.l2_loss(I * (p_x - x)/(image_size/cell_size)) +\n",
    "                 tf.nn.l2_loss(I * (p_y - y)/(image_size/cell_size)) +\n",
    "                 tf.nn.l2_loss(I * (p_sqrt_w - sqrt_w))/ image_size +\n",
    "                 tf.nn.l2_loss(I * (p_sqrt_h - sqrt_h))/image_size) * coord_scale\n",
    "\n",
    "    nilboy = I\n",
    "\n",
    "    return num + 1, object_num, [loss[0] + class_loss, loss[1] + object_loss, loss[2] + noobject_loss, loss[3] + coord_loss], predict, labels, nilboy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(predicts, labels, objects_num):\n",
    "    \"\"\"Add Loss to all the trainable variables\n",
    "\n",
    "    Args:\n",
    "      predicts: 4-D tensor [batch_size, cell_size, cell_size, 5 * boxes_per_cell]\n",
    "      ===> (num_classes, boxes_per_cell, 4 * boxes_per_cell)\n",
    "      labels  : 3-D tensor of [batch_size, max_objects, 5]\n",
    "      objects_num: 1-D tensor [batch_size]\n",
    "    \"\"\"\n",
    "    class_loss = tf.constant(0, tf.float32)\n",
    "    object_loss = tf.constant(0, tf.float32)\n",
    "    noobject_loss = tf.constant(0, tf.float32)\n",
    "    coord_loss = tf.constant(0, tf.float32)\n",
    "    loss = [0, 0, 0, 0]\n",
    "    for i in range(batch_size):\n",
    "        predict = predicts[i, :, :, :]\n",
    "        label = labels[i, :, :]\n",
    "        object_num = objects_num[i]\n",
    "        nilboy = tf.ones([7,7,2])\n",
    "        tuple_results = tf.while_loop(cond1, body1, [tf.constant(0), object_num, [class_loss, object_loss, noobject_loss, coord_loss], predict, label, nilboy])\n",
    "        for j in range(4):\n",
    "            loss[j] = loss[j] + tuple_results[2][j]\n",
    "        nilboy = tuple_results[5]\n",
    "\n",
    "    tf.add_to_collection('losses', (loss[0] + loss[1] + loss[2] + loss[3]) / batch_size)\n",
    "\n",
    "#     tf.summary.scalar('class_loss', loss[0]/self.batch_size)\n",
    "#     tf.summary.scalar('object_loss', loss[1]/self.batch_size)\n",
    "#     tf.summary.scalar('noobject_loss', loss[2]/self.batch_size)\n",
    "#     tf.summary.scalar('coord_loss', loss[3]/self.batch_size)\n",
    "#     tf.summary.scalar('weight_loss', tf.add_n(tf.get_collection('losses')) - (loss[0] + loss[1] + loss[2] + loss[3])/self.batch_size )\n",
    "\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss'), nilboy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_size = 448\n",
    "batch_size = 16\n",
    "num_classes = 20\n",
    "max_objects_per_image = 20\n",
    "width = image_size\n",
    "height = image_size\n",
    "max_objects = max_objects_per_image\n",
    "cell_size = 7\n",
    "boxes_per_cell = 2\n",
    "object_scale = 1\n",
    "noobject_scale = 0.5\n",
    "class_scale = 1\n",
    "coord_scale = 5\n",
    "learning_rate = 0.000000001\n",
    "max_iterators = 10000\n",
    "\n",
    "images = tf.placeholder(tf.float32, (None, height, width, 3))\n",
    "labels = tf.placeholder(tf.float32, (None, max_objects, 5))\n",
    "objects_num = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    conv_num = 1\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), images, [3, 3, 3, 16], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_pool = max_pool(temp_conv, [2, 2], 2)\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_pool, [3, 3, 16, 32], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_pool = max_pool(temp_conv, [2, 2], 2)\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_pool, [3, 3, 32, 64], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = max_pool(temp_conv, [2, 2], 2)\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 64, 128], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = max_pool(temp_conv, [2, 2], 2)\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 128, 256], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = max_pool(temp_conv, [2, 2], 2)\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 256, 512], stride=1)\n",
    "    conv_num += 1\n",
    "\n",
    "    temp_conv = max_pool(temp_conv, [2, 2], 2)\n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 512, 1024], stride=1)\n",
    "    conv_num += 1     \n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 1024, 1024], stride=1)\n",
    "    conv_num += 1 \n",
    "\n",
    "    temp_conv = conv2d('conv' + str(conv_num), temp_conv, [3, 3, 1024, 1024], stride=1)\n",
    "    conv_num += 1 \n",
    "\n",
    "    temp_conv = tf.transpose(temp_conv, (0, 3, 1, 2))\n",
    "\n",
    "    #Fully connected layer\n",
    "    local1 = local('local1', temp_conv, cell_size * cell_size * 1024, 256)\n",
    "\n",
    "    local2 = local('local2', local1, 256, 4096)\n",
    "\n",
    "    local3 = local('local3', local2, 4096, cell_size * cell_size * (num_classes + boxes_per_cell * 5), leaky=False)\n",
    "\n",
    "n1 = cell_size * cell_size * num_classes\n",
    "\n",
    "n2 = n1 + cell_size * cell_size * boxes_per_cell\n",
    "\n",
    "class_probs = tf.reshape(local3[:, 0:n1], (-1, cell_size, cell_size, num_classes))\n",
    "scales = tf.reshape(local3[:, n1:n2], (-1, cell_size, cell_size, boxes_per_cell))\n",
    "boxes = tf.reshape(local3[:, n2:], (-1, cell_size, cell_size, boxes_per_cell * 4))\n",
    "\n",
    "# local3 = tf.concat([class_probs, scales, boxes], 3)\n",
    "\n",
    "# n1 = cell_size * cell_size * num_classes\n",
    "\n",
    "# n2 = n1 + cell_size * cell_size * boxes_per_cell\n",
    "\n",
    "# class_probs = tf.reshape(local3[:, 0:n1], (-1, cell_size, cell_size, num_classes))\n",
    "# scales = tf.reshape(local3[:, n1:n2], (-1, cell_size, cell_size, boxes_per_cell))\n",
    "# boxes = tf.reshape(local3[:, n2:], (-1, cell_size, cell_size, boxes_per_cell * 4))\n",
    "\n",
    "predicts = tf.concat([class_probs, scales, boxes], 3)\n",
    "\n",
    "total_loss, nilboy = loss(predicts, labels, objects_num)\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(total_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cvtColor(image):\n",
    "    arr = np.array(image)\n",
    "    tmp = arr[:, :, 0].copy()\n",
    "    arr[:, :, 0] = arr[:, :, 2]\n",
    "    arr[:, :, 2] = tmp\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def record_process(record):\n",
    "    \"\"\"record process \n",
    "    Args: record \n",
    "    Returns:\n",
    "      image: 3-D ndarray\n",
    "      labels: 2-D list [self.max_objects, 5] (xcenter, ycenter, w, h, class_num)\n",
    "      object_num:  total object number  int \n",
    "    \"\"\"\n",
    "    global width\n",
    "    global height\n",
    "    global max_objects\n",
    "    \n",
    "    image = Image.open(record[0])\n",
    "    image = cvtColor(image)\n",
    "#     image = cv2.imread(record[0])\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]\n",
    "\n",
    "    width_rate = width * 1.0 / w \n",
    "    height_rate = height * 1.0 / h \n",
    "    \n",
    "    image = Image.fromarray(image)\n",
    "    image = image.resize((height, width))\n",
    "#     image = cv2.resize(image, (height, width))\n",
    "\n",
    "    labels = [[0, 0, 0, 0, 0]] * max_objects\n",
    "    i = 1\n",
    "    object_num = 0\n",
    "    while i < len(record):\n",
    "        xmin = record[i]\n",
    "        ymin = record[i + 1]\n",
    "        xmax = record[i + 2]\n",
    "        ymax = record[i + 3]\n",
    "        class_num = record[i + 4]\n",
    "\n",
    "        xcenter = (xmin + xmax) * 1.0 / 2 * width_rate\n",
    "        ycenter = (ymin + ymax) * 1.0 / 2 * height_rate\n",
    "\n",
    "        box_w = (xmax - xmin) * width_rate\n",
    "        box_h = (ymax - ymin) * height_rate\n",
    "\n",
    "        labels[object_num] = [xcenter, ycenter, box_w, box_h, class_num]\n",
    "        object_num += 1\n",
    "        i += 5\n",
    "        if object_num >= max_objects:\n",
    "            break\n",
    "    return image, labels, object_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'yolo2d_data/pascal_voc.txt'\n",
    "record_list = []  \n",
    "\n",
    "# filling the record_list\n",
    "input_file = open(data_path, 'r')\n",
    "\n",
    "for line in input_file:\n",
    "    line = line.strip()\n",
    "    ss = line.split(' ')\n",
    "    ss[1:] = [float(num) for num in ss[1:]]\n",
    "    record_list.append(ss)\n",
    "\n",
    "record_point = 0\n",
    "record_number = len(record_list)\n",
    "num_batch_per_epoch = int(np.ceil(record_number / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from yolo2d_model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saveDir = 'yolo2d_model'\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, saveDir + '/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init =  tf.global_variables_initializer()\n",
    "# sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-21 10:06:55.906690: step 0 batch 0, loss = 10.79 (2.3 examples/sec; 6.894 sec/batch)\n",
      "2017-06-21 10:07:20.892076: step 0 batch 10, loss = 10.86 (0.5 examples/sec; 31.880 sec/batch)\n",
      "2017-06-21 10:07:45.681689: step 0 batch 20, loss = 11.76 (0.3 examples/sec; 56.669 sec/batch)\n",
      "2017-06-21 10:08:10.404940: step 0 batch 30, loss = 8.29 (0.2 examples/sec; 81.393 sec/batch)\n",
      "2017-06-21 10:08:35.229592: step 0 batch 40, loss = 10.67 (0.2 examples/sec; 106.217 sec/batch)\n",
      "2017-06-21 10:09:00.081252: step 0 batch 50, loss = 13.79 (0.1 examples/sec; 131.069 sec/batch)\n",
      "2017-06-21 10:09:24.980310: step 0 batch 60, loss = 10.40 (0.1 examples/sec; 155.968 sec/batch)\n",
      "2017-06-21 10:09:49.802053: step 0 batch 70, loss = 11.35 (0.1 examples/sec; 180.790 sec/batch)\n",
      "2017-06-21 10:10:14.395420: step 0 batch 80, loss = 9.41 (0.1 examples/sec; 205.383 sec/batch)\n",
      "2017-06-21 10:10:39.095732: step 0 batch 90, loss = 11.52 (0.1 examples/sec; 230.083 sec/batch)\n",
      "2017-06-21 10:11:04.160971: step 0 batch 100, loss = 8.56 (0.1 examples/sec; 255.149 sec/batch)\n",
      "2017-06-21 10:11:29.106659: step 0 batch 110, loss = 12.73 (0.1 examples/sec; 280.094 sec/batch)\n",
      "2017-06-21 10:11:53.979981: step 0 batch 120, loss = 14.41 (0.1 examples/sec; 304.968 sec/batch)\n",
      "2017-06-21 10:12:18.793464: step 0 batch 130, loss = 10.17 (0.0 examples/sec; 329.781 sec/batch)\n",
      "2017-06-21 10:12:43.642482: step 0 batch 140, loss = 10.94 (0.0 examples/sec; 354.630 sec/batch)\n",
      "2017-06-21 10:13:08.719788: step 0 batch 150, loss = 10.95 (0.0 examples/sec; 379.708 sec/batch)\n",
      "2017-06-21 10:13:33.586001: step 0 batch 160, loss = 13.16 (0.0 examples/sec; 404.574 sec/batch)\n",
      "2017-06-21 10:13:58.348490: step 0 batch 170, loss = 8.08 (0.0 examples/sec; 429.336 sec/batch)\n",
      "2017-06-21 10:14:23.220867: step 0 batch 180, loss = 10.41 (0.0 examples/sec; 454.209 sec/batch)\n",
      "2017-06-21 10:14:48.012955: step 0 batch 190, loss = 15.28 (0.0 examples/sec; 479.001 sec/batch)\n",
      "2017-06-21 10:15:12.717914: step 0 batch 200, loss = 8.99 (0.0 examples/sec; 503.706 sec/batch)\n",
      "2017-06-21 10:15:37.354430: step 0 batch 210, loss = 8.48 (0.0 examples/sec; 528.342 sec/batch)\n",
      "2017-06-21 10:16:02.005926: step 0 batch 220, loss = 11.53 (0.0 examples/sec; 552.994 sec/batch)\n",
      "2017-06-21 10:16:26.969876: step 0 batch 230, loss = 14.02 (0.0 examples/sec; 577.958 sec/batch)\n",
      "2017-06-21 10:16:51.630775: step 0 batch 240, loss = 12.89 (0.0 examples/sec; 602.618 sec/batch)\n",
      "2017-06-21 10:17:16.409649: step 0 batch 250, loss = 19.46 (0.0 examples/sec; 627.397 sec/batch)\n",
      "2017-06-21 10:17:41.261366: step 0 batch 260, loss = 12.03 (0.0 examples/sec; 652.249 sec/batch)\n",
      "2017-06-21 10:18:05.912059: step 0 batch 270, loss = 9.77 (0.0 examples/sec; 676.900 sec/batch)\n",
      "2017-06-21 10:18:30.671816: step 0 batch 280, loss = 10.95 (0.0 examples/sec; 701.660 sec/batch)\n",
      "2017-06-21 10:18:55.369099: step 0 batch 290, loss = 12.29 (0.0 examples/sec; 726.357 sec/batch)\n",
      "2017-06-21 10:19:20.186512: step 0 batch 300, loss = 12.19 (0.0 examples/sec; 751.174 sec/batch)\n",
      "2017-06-21 10:19:45.098847: step 0 batch 310, loss = 12.30 (0.0 examples/sec; 776.087 sec/batch)\n",
      "2017-06-21 10:20:09.851881: step 0 batch 320, loss = 12.55 (0.0 examples/sec; 800.840 sec/batch)\n",
      "2017-06-21 10:20:34.599155: step 0 batch 330, loss = 15.02 (0.0 examples/sec; 825.587 sec/batch)\n",
      "2017-06-21 10:20:59.366526: step 0 batch 340, loss = 14.65 (0.0 examples/sec; 850.354 sec/batch)\n",
      "2017-06-21 10:21:24.276445: step 0 batch 350, loss = 14.53 (0.0 examples/sec; 875.264 sec/batch)\n",
      "2017-06-21 10:21:49.059919: step 0 batch 360, loss = 13.14 (0.0 examples/sec; 900.048 sec/batch)\n",
      "2017-06-21 10:22:13.975403: step 0 batch 370, loss = 13.68 (0.0 examples/sec; 924.963 sec/batch)\n",
      "2017-06-21 10:22:38.732244: step 0 batch 380, loss = 13.36 (0.0 examples/sec; 949.720 sec/batch)\n",
      "2017-06-21 10:23:03.502270: step 0 batch 390, loss = 13.28 (0.0 examples/sec; 974.490 sec/batch)\n",
      "2017-06-21 10:23:28.146951: step 0 batch 400, loss = 9.24 (0.0 examples/sec; 999.135 sec/batch)\n",
      "2017-06-21 10:23:52.890080: step 0 batch 410, loss = 9.21 (0.0 examples/sec; 1023.878 sec/batch)\n",
      "2017-06-21 10:24:17.513057: step 0 batch 420, loss = 9.21 (0.0 examples/sec; 1048.501 sec/batch)\n",
      "2017-06-21 10:24:42.125993: step 0 batch 430, loss = 13.81 (0.0 examples/sec; 1073.114 sec/batch)\n",
      "2017-06-21 10:25:06.827695: step 0 batch 440, loss = 13.17 (0.0 examples/sec; 1097.815 sec/batch)\n",
      "2017-06-21 10:25:31.593550: step 0 batch 450, loss = 11.34 (0.0 examples/sec; 1122.581 sec/batch)\n",
      "2017-06-21 10:25:56.440329: step 0 batch 460, loss = 4.98 (0.0 examples/sec; 1147.428 sec/batch)\n",
      "2017-06-21 10:26:20.972784: step 0 batch 470, loss = 14.14 (0.0 examples/sec; 1171.961 sec/batch)\n",
      "2017-06-21 10:26:45.678666: step 0 batch 480, loss = 13.64 (0.0 examples/sec; 1196.666 sec/batch)\n",
      "2017-06-21 10:27:10.193630: step 0 batch 490, loss = 12.46 (0.0 examples/sec; 1221.181 sec/batch)\n",
      "2017-06-21 10:27:34.811768: step 0 batch 500, loss = 15.71 (0.0 examples/sec; 1245.800 sec/batch)\n",
      "2017-06-21 10:27:59.415667: step 0 batch 510, loss = 14.22 (0.0 examples/sec; 1270.403 sec/batch)\n",
      "2017-06-21 10:28:24.009412: step 0 batch 520, loss = 11.95 (0.0 examples/sec; 1294.997 sec/batch)\n",
      "2017-06-21 10:28:48.574233: step 0 batch 530, loss = 11.90 (0.0 examples/sec; 1319.562 sec/batch)\n",
      "2017-06-21 10:29:13.386754: step 0 batch 540, loss = 14.09 (0.0 examples/sec; 1344.374 sec/batch)\n",
      "2017-06-21 10:29:38.297579: step 0 batch 550, loss = 12.88 (0.0 examples/sec; 1369.285 sec/batch)\n",
      "2017-06-21 10:30:03.028322: step 0 batch 560, loss = 11.28 (0.0 examples/sec; 1394.016 sec/batch)\n",
      "2017-06-21 10:30:27.659577: step 0 batch 570, loss = 9.55 (0.0 examples/sec; 1418.647 sec/batch)\n",
      "2017-06-21 10:30:52.396394: step 0 batch 580, loss = 12.21 (0.0 examples/sec; 1443.384 sec/batch)\n",
      "2017-06-21 10:31:17.014685: step 0 batch 590, loss = 8.79 (0.0 examples/sec; 1468.002 sec/batch)\n",
      "2017-06-21 10:31:41.669153: step 0 batch 600, loss = 16.82 (0.0 examples/sec; 1492.657 sec/batch)\n",
      "2017-06-21 10:32:06.589493: step 0 batch 610, loss = 10.42 (0.0 examples/sec; 1517.577 sec/batch)\n",
      "2017-06-21 10:32:21.202951: step 1 batch 0, loss = 11.41 (6.8 examples/sec; 2.342 sec/batch)\n",
      "2017-06-21 10:32:44.821932: step 1 batch 10, loss = 10.04 (0.6 examples/sec; 25.961 sec/batch)\n",
      "2017-06-21 10:33:08.403751: step 1 batch 20, loss = 14.14 (0.3 examples/sec; 49.543 sec/batch)\n",
      "2017-06-21 10:33:32.134661: step 1 batch 30, loss = 9.58 (0.2 examples/sec; 73.274 sec/batch)\n",
      "2017-06-21 10:33:55.899000: step 1 batch 40, loss = 13.86 (0.2 examples/sec; 97.038 sec/batch)\n",
      "2017-06-21 10:34:19.545099: step 1 batch 50, loss = 11.71 (0.1 examples/sec; 120.684 sec/batch)\n",
      "2017-06-21 10:34:43.116794: step 1 batch 60, loss = 13.92 (0.1 examples/sec; 144.256 sec/batch)\n",
      "2017-06-21 10:35:06.787350: step 1 batch 70, loss = 9.28 (0.1 examples/sec; 167.926 sec/batch)\n",
      "2017-06-21 10:35:30.390089: step 1 batch 80, loss = 10.11 (0.1 examples/sec; 191.529 sec/batch)\n",
      "2017-06-21 10:35:54.069804: step 1 batch 90, loss = 11.69 (0.1 examples/sec; 215.209 sec/batch)\n",
      "2017-06-21 10:36:17.476092: step 1 batch 100, loss = 10.01 (0.1 examples/sec; 238.615 sec/batch)\n",
      "2017-06-21 10:36:41.015786: step 1 batch 110, loss = 10.96 (0.1 examples/sec; 262.155 sec/batch)\n",
      "2017-06-21 10:37:04.621145: step 1 batch 120, loss = 14.24 (0.1 examples/sec; 285.760 sec/batch)\n",
      "2017-06-21 10:37:28.316409: step 1 batch 130, loss = 11.77 (0.1 examples/sec; 309.455 sec/batch)\n",
      "2017-06-21 10:37:51.897547: step 1 batch 140, loss = 15.71 (0.0 examples/sec; 333.036 sec/batch)\n",
      "2017-06-21 10:38:15.415622: step 1 batch 150, loss = 15.45 (0.0 examples/sec; 356.554 sec/batch)\n",
      "2017-06-21 10:38:38.947313: step 1 batch 160, loss = 12.99 (0.0 examples/sec; 380.086 sec/batch)\n",
      "2017-06-21 10:39:02.504394: step 1 batch 170, loss = 10.73 (0.0 examples/sec; 403.643 sec/batch)\n",
      "2017-06-21 10:39:26.245661: step 1 batch 180, loss = 15.50 (0.0 examples/sec; 427.385 sec/batch)\n",
      "2017-06-21 10:39:49.838840: step 1 batch 190, loss = 11.60 (0.0 examples/sec; 450.978 sec/batch)\n",
      "2017-06-21 10:40:13.536930: step 1 batch 200, loss = 11.40 (0.0 examples/sec; 474.676 sec/batch)\n",
      "2017-06-21 10:40:37.224416: step 1 batch 210, loss = 14.79 (0.0 examples/sec; 498.363 sec/batch)\n",
      "2017-06-21 10:41:00.937371: step 1 batch 220, loss = 10.45 (0.0 examples/sec; 522.076 sec/batch)\n",
      "2017-06-21 10:41:24.563781: step 1 batch 230, loss = 13.62 (0.0 examples/sec; 545.703 sec/batch)\n",
      "2017-06-21 10:41:48.091184: step 1 batch 240, loss = 11.27 (0.0 examples/sec; 569.230 sec/batch)\n",
      "2017-06-21 10:42:11.803800: step 1 batch 250, loss = 13.86 (0.0 examples/sec; 592.943 sec/batch)\n",
      "2017-06-21 10:42:35.440471: step 1 batch 260, loss = 7.16 (0.0 examples/sec; 616.579 sec/batch)\n",
      "2017-06-21 10:42:59.042800: step 1 batch 270, loss = 12.95 (0.0 examples/sec; 640.182 sec/batch)\n",
      "2017-06-21 10:43:22.755365: step 1 batch 280, loss = 12.01 (0.0 examples/sec; 663.894 sec/batch)\n",
      "2017-06-21 10:43:46.288884: step 1 batch 290, loss = 13.23 (0.0 examples/sec; 687.428 sec/batch)\n",
      "2017-06-21 10:44:09.934854: step 1 batch 300, loss = 10.95 (0.0 examples/sec; 711.074 sec/batch)\n",
      "2017-06-21 10:44:33.394304: step 1 batch 310, loss = 9.68 (0.0 examples/sec; 734.533 sec/batch)\n",
      "2017-06-21 10:44:56.923657: step 1 batch 320, loss = 14.40 (0.0 examples/sec; 758.062 sec/batch)\n",
      "2017-06-21 10:45:20.467888: step 1 batch 330, loss = 11.06 (0.0 examples/sec; 781.607 sec/batch)\n",
      "2017-06-21 10:45:43.990360: step 1 batch 340, loss = 8.31 (0.0 examples/sec; 805.129 sec/batch)\n",
      "2017-06-21 10:46:07.530230: step 1 batch 350, loss = 9.71 (0.0 examples/sec; 828.669 sec/batch)\n",
      "2017-06-21 10:46:31.013769: step 1 batch 360, loss = 7.94 (0.0 examples/sec; 852.153 sec/batch)\n",
      "2017-06-21 10:46:54.637411: step 1 batch 370, loss = 12.71 (0.0 examples/sec; 875.776 sec/batch)\n",
      "2017-06-21 10:47:18.248097: step 1 batch 380, loss = 13.13 (0.0 examples/sec; 899.387 sec/batch)\n",
      "2017-06-21 10:47:41.820705: step 1 batch 390, loss = 11.75 (0.0 examples/sec; 922.960 sec/batch)\n",
      "2017-06-21 10:48:05.684599: step 1 batch 400, loss = 11.05 (0.0 examples/sec; 946.823 sec/batch)\n",
      "2017-06-21 10:48:29.291483: step 1 batch 410, loss = 12.26 (0.0 examples/sec; 970.430 sec/batch)\n",
      "2017-06-21 10:48:52.726135: step 1 batch 420, loss = 15.68 (0.0 examples/sec; 993.865 sec/batch)\n",
      "2017-06-21 10:49:16.267549: step 1 batch 430, loss = 9.55 (0.0 examples/sec; 1017.406 sec/batch)\n",
      "2017-06-21 10:49:40.012650: step 1 batch 440, loss = 12.84 (0.0 examples/sec; 1041.151 sec/batch)\n",
      "2017-06-21 10:50:03.718091: step 1 batch 450, loss = 8.39 (0.0 examples/sec; 1064.857 sec/batch)\n",
      "2017-06-21 10:50:27.362578: step 1 batch 460, loss = 13.63 (0.0 examples/sec; 1088.501 sec/batch)\n",
      "2017-06-21 10:50:51.130750: step 1 batch 470, loss = 12.76 (0.0 examples/sec; 1112.270 sec/batch)\n",
      "2017-06-21 10:51:14.552651: step 1 batch 480, loss = 11.81 (0.0 examples/sec; 1135.691 sec/batch)\n",
      "2017-06-21 10:51:38.178025: step 1 batch 490, loss = 9.47 (0.0 examples/sec; 1159.317 sec/batch)\n",
      "2017-06-21 10:52:01.760111: step 1 batch 500, loss = 8.85 (0.0 examples/sec; 1182.899 sec/batch)\n",
      "2017-06-21 10:52:25.258837: step 1 batch 510, loss = 10.90 (0.0 examples/sec; 1206.398 sec/batch)\n",
      "2017-06-21 10:52:48.914895: step 1 batch 520, loss = 10.99 (0.0 examples/sec; 1230.054 sec/batch)\n",
      "2017-06-21 10:53:12.590915: step 1 batch 530, loss = 10.25 (0.0 examples/sec; 1253.730 sec/batch)\n",
      "2017-06-21 10:53:36.186701: step 1 batch 540, loss = 11.77 (0.0 examples/sec; 1277.326 sec/batch)\n",
      "2017-06-21 10:53:59.770434: step 1 batch 550, loss = 8.95 (0.0 examples/sec; 1300.909 sec/batch)\n",
      "2017-06-21 10:54:23.447131: step 1 batch 560, loss = 9.74 (0.0 examples/sec; 1324.586 sec/batch)\n",
      "2017-06-21 10:54:47.017367: step 1 batch 570, loss = 12.22 (0.0 examples/sec; 1348.156 sec/batch)\n",
      "2017-06-21 10:55:10.599873: step 1 batch 580, loss = 8.22 (0.0 examples/sec; 1371.739 sec/batch)\n",
      "2017-06-21 10:55:34.230976: step 1 batch 590, loss = 11.15 (0.0 examples/sec; 1395.370 sec/batch)\n",
      "2017-06-21 10:55:57.873540: step 1 batch 600, loss = 10.52 (0.0 examples/sec; 1419.012 sec/batch)\n",
      "2017-06-21 10:56:21.585459: step 1 batch 610, loss = 10.77 (0.0 examples/sec; 1442.724 sec/batch)\n",
      "2017-06-21 10:56:35.710459: step 2 batch 0, loss = 10.87 (6.7 examples/sec; 2.384 sec/batch)\n",
      "2017-06-21 10:56:59.249341: step 2 batch 10, loss = 12.22 (0.6 examples/sec; 25.923 sec/batch)\n",
      "2017-06-21 10:57:22.884447: step 2 batch 20, loss = 11.16 (0.3 examples/sec; 49.558 sec/batch)\n",
      "2017-06-21 10:57:46.311403: step 2 batch 30, loss = 10.51 (0.2 examples/sec; 72.985 sec/batch)\n",
      "2017-06-21 10:58:09.955313: step 2 batch 40, loss = 17.03 (0.2 examples/sec; 96.629 sec/batch)\n",
      "2017-06-21 10:58:33.689676: step 2 batch 50, loss = 8.69 (0.1 examples/sec; 120.363 sec/batch)\n",
      "2017-06-21 10:58:57.427421: step 2 batch 60, loss = 10.29 (0.1 examples/sec; 144.101 sec/batch)\n",
      "2017-06-21 10:59:21.220178: step 2 batch 70, loss = 13.04 (0.1 examples/sec; 167.894 sec/batch)\n",
      "2017-06-21 10:59:44.972433: step 2 batch 80, loss = 12.75 (0.1 examples/sec; 191.646 sec/batch)\n",
      "2017-06-21 11:00:08.654616: step 2 batch 90, loss = 10.58 (0.1 examples/sec; 215.328 sec/batch)\n",
      "2017-06-21 11:00:32.119877: step 2 batch 100, loss = 12.76 (0.1 examples/sec; 238.793 sec/batch)\n",
      "2017-06-21 11:00:55.824804: step 2 batch 110, loss = 13.66 (0.1 examples/sec; 262.498 sec/batch)\n",
      "2017-06-21 11:01:19.408716: step 2 batch 120, loss = 10.58 (0.1 examples/sec; 286.082 sec/batch)\n",
      "2017-06-21 11:01:42.770602: step 2 batch 130, loss = 9.84 (0.1 examples/sec; 309.444 sec/batch)\n",
      "2017-06-21 11:02:06.357384: step 2 batch 140, loss = 9.50 (0.0 examples/sec; 333.031 sec/batch)\n",
      "2017-06-21 11:02:29.959292: step 2 batch 150, loss = 12.69 (0.0 examples/sec; 356.633 sec/batch)\n",
      "2017-06-21 11:02:53.525991: step 2 batch 160, loss = 10.96 (0.0 examples/sec; 380.199 sec/batch)\n",
      "2017-06-21 11:03:17.062165: step 2 batch 170, loss = 11.24 (0.0 examples/sec; 403.736 sec/batch)\n",
      "2017-06-21 11:03:40.666035: step 2 batch 180, loss = 12.57 (0.0 examples/sec; 427.340 sec/batch)\n",
      "2017-06-21 11:04:04.449064: step 2 batch 190, loss = 7.18 (0.0 examples/sec; 451.123 sec/batch)\n",
      "2017-06-21 11:04:28.187225: step 2 batch 200, loss = 18.26 (0.0 examples/sec; 474.861 sec/batch)\n",
      "2017-06-21 11:04:51.691276: step 2 batch 210, loss = 11.06 (0.0 examples/sec; 498.365 sec/batch)\n",
      "2017-06-21 11:05:15.426179: step 2 batch 220, loss = 8.94 (0.0 examples/sec; 522.100 sec/batch)\n",
      "2017-06-21 11:05:38.920473: step 2 batch 230, loss = 7.84 (0.0 examples/sec; 545.594 sec/batch)\n",
      "2017-06-21 11:06:02.595401: step 2 batch 240, loss = 8.92 (0.0 examples/sec; 569.269 sec/batch)\n",
      "2017-06-21 11:06:26.081300: step 2 batch 250, loss = 11.86 (0.0 examples/sec; 592.755 sec/batch)\n",
      "2017-06-21 11:06:49.552689: step 2 batch 260, loss = 5.12 (0.0 examples/sec; 616.226 sec/batch)\n",
      "2017-06-21 11:07:13.317185: step 2 batch 270, loss = 11.94 (0.0 examples/sec; 639.991 sec/batch)\n",
      "2017-06-21 11:07:36.953118: step 2 batch 280, loss = 9.12 (0.0 examples/sec; 663.627 sec/batch)\n",
      "2017-06-21 11:08:00.610143: step 2 batch 290, loss = 9.75 (0.0 examples/sec; 687.284 sec/batch)\n",
      "2017-06-21 11:08:24.101254: step 2 batch 300, loss = 13.13 (0.0 examples/sec; 710.775 sec/batch)\n",
      "2017-06-21 11:08:47.597953: step 2 batch 310, loss = 12.38 (0.0 examples/sec; 734.271 sec/batch)\n",
      "2017-06-21 11:09:11.125817: step 2 batch 320, loss = 12.32 (0.0 examples/sec; 757.799 sec/batch)\n",
      "2017-06-21 11:09:34.638004: step 2 batch 330, loss = 11.64 (0.0 examples/sec; 781.311 sec/batch)\n",
      "2017-06-21 11:09:58.170997: step 2 batch 340, loss = 15.40 (0.0 examples/sec; 804.844 sec/batch)\n",
      "2017-06-21 11:10:21.738419: step 2 batch 350, loss = 7.65 (0.0 examples/sec; 828.412 sec/batch)\n",
      "2017-06-21 11:10:45.522859: step 2 batch 360, loss = 13.49 (0.0 examples/sec; 852.196 sec/batch)\n",
      "2017-06-21 11:11:08.997821: step 2 batch 370, loss = 12.11 (0.0 examples/sec; 875.671 sec/batch)\n",
      "2017-06-21 11:11:32.756907: step 2 batch 380, loss = 15.03 (0.0 examples/sec; 899.430 sec/batch)\n",
      "2017-06-21 11:11:56.334694: step 2 batch 390, loss = 11.12 (0.0 examples/sec; 923.008 sec/batch)\n",
      "2017-06-21 11:12:20.020220: step 2 batch 400, loss = 11.03 (0.0 examples/sec; 946.694 sec/batch)\n",
      "2017-06-21 11:12:43.758931: step 2 batch 410, loss = 17.45 (0.0 examples/sec; 970.432 sec/batch)\n",
      "2017-06-21 11:13:07.451418: step 2 batch 420, loss = 14.81 (0.0 examples/sec; 994.125 sec/batch)\n",
      "2017-06-21 11:13:31.080379: step 2 batch 430, loss = 10.70 (0.0 examples/sec; 1017.754 sec/batch)\n",
      "2017-06-21 11:13:54.746990: step 2 batch 440, loss = 13.54 (0.0 examples/sec; 1041.420 sec/batch)\n",
      "2017-06-21 11:14:18.308606: step 2 batch 450, loss = 11.71 (0.0 examples/sec; 1064.982 sec/batch)\n",
      "2017-06-21 11:14:41.846973: step 2 batch 460, loss = 11.38 (0.0 examples/sec; 1088.520 sec/batch)\n",
      "2017-06-21 11:15:05.460994: step 2 batch 470, loss = 12.87 (0.0 examples/sec; 1112.134 sec/batch)\n",
      "2017-06-21 11:15:29.047419: step 2 batch 480, loss = 16.50 (0.0 examples/sec; 1135.721 sec/batch)\n",
      "2017-06-21 11:15:52.576548: step 2 batch 490, loss = 11.04 (0.0 examples/sec; 1159.250 sec/batch)\n",
      "2017-06-21 11:16:15.999989: step 2 batch 500, loss = 12.06 (0.0 examples/sec; 1182.673 sec/batch)\n",
      "2017-06-21 11:16:39.603444: step 2 batch 510, loss = 11.11 (0.0 examples/sec; 1206.277 sec/batch)\n",
      "2017-06-21 11:17:03.203789: step 2 batch 520, loss = 16.26 (0.0 examples/sec; 1229.877 sec/batch)\n",
      "2017-06-21 11:17:26.774184: step 2 batch 530, loss = 13.04 (0.0 examples/sec; 1253.448 sec/batch)\n",
      "2017-06-21 11:17:50.553720: step 2 batch 540, loss = 14.74 (0.0 examples/sec; 1277.227 sec/batch)\n",
      "2017-06-21 11:18:14.172544: step 2 batch 550, loss = 9.02 (0.0 examples/sec; 1300.846 sec/batch)\n",
      "2017-06-21 11:18:37.804295: step 2 batch 560, loss = 9.41 (0.0 examples/sec; 1324.478 sec/batch)\n",
      "2017-06-21 11:19:01.305928: step 2 batch 570, loss = 10.03 (0.0 examples/sec; 1347.979 sec/batch)\n",
      "2017-06-21 11:19:24.854568: step 2 batch 580, loss = 12.81 (0.0 examples/sec; 1371.528 sec/batch)\n",
      "2017-06-21 11:19:48.646292: step 2 batch 590, loss = 15.59 (0.0 examples/sec; 1395.320 sec/batch)\n",
      "2017-06-21 11:20:12.349156: step 2 batch 600, loss = 12.18 (0.0 examples/sec; 1419.023 sec/batch)\n",
      "2017-06-21 11:20:35.845379: step 2 batch 610, loss = 8.35 (0.0 examples/sec; 1442.519 sec/batch)\n",
      "2017-06-21 11:20:49.892459: step 3 batch 0, loss = 12.73 (6.9 examples/sec; 2.335 sec/batch)\n",
      "2017-06-21 11:21:13.457995: step 3 batch 10, loss = 9.09 (0.6 examples/sec; 25.901 sec/batch)\n",
      "2017-06-21 11:21:37.006881: step 3 batch 20, loss = 9.11 (0.3 examples/sec; 49.450 sec/batch)\n",
      "2017-06-21 11:22:00.705601: step 3 batch 30, loss = 10.35 (0.2 examples/sec; 73.148 sec/batch)\n",
      "2017-06-21 11:22:24.158889: step 3 batch 40, loss = 13.63 (0.2 examples/sec; 96.602 sec/batch)\n",
      "2017-06-21 11:22:47.590674: step 3 batch 50, loss = 14.27 (0.1 examples/sec; 120.033 sec/batch)\n",
      "2017-06-21 11:23:11.363397: step 3 batch 60, loss = 13.63 (0.1 examples/sec; 143.806 sec/batch)\n",
      "2017-06-21 11:23:34.919672: step 3 batch 70, loss = 9.42 (0.1 examples/sec; 167.362 sec/batch)\n",
      "2017-06-21 11:23:58.498956: step 3 batch 80, loss = 10.63 (0.1 examples/sec; 190.942 sec/batch)\n",
      "2017-06-21 11:24:22.136588: step 3 batch 90, loss = 11.27 (0.1 examples/sec; 214.579 sec/batch)\n",
      "2017-06-21 11:24:45.666661: step 3 batch 100, loss = 9.21 (0.1 examples/sec; 238.109 sec/batch)\n",
      "2017-06-21 11:25:09.425377: step 3 batch 110, loss = 12.54 (0.1 examples/sec; 261.868 sec/batch)\n",
      "2017-06-21 11:25:33.038722: step 3 batch 120, loss = 14.07 (0.1 examples/sec; 285.481 sec/batch)\n",
      "2017-06-21 11:25:56.617537: step 3 batch 130, loss = 12.78 (0.1 examples/sec; 309.060 sec/batch)\n",
      "2017-06-21 11:26:20.360981: step 3 batch 140, loss = 10.68 (0.0 examples/sec; 332.804 sec/batch)\n",
      "2017-06-21 11:26:44.009469: step 3 batch 150, loss = 9.53 (0.0 examples/sec; 356.452 sec/batch)\n",
      "2017-06-21 11:27:07.777236: step 3 batch 160, loss = 11.11 (0.0 examples/sec; 380.220 sec/batch)\n",
      "2017-06-21 11:27:31.544455: step 3 batch 170, loss = 9.29 (0.0 examples/sec; 403.987 sec/batch)\n",
      "2017-06-21 11:27:55.206022: step 3 batch 180, loss = 9.36 (0.0 examples/sec; 427.649 sec/batch)\n",
      "2017-06-21 11:28:18.740997: step 3 batch 190, loss = 16.24 (0.0 examples/sec; 451.184 sec/batch)\n",
      "2017-06-21 11:28:42.290565: step 3 batch 200, loss = 13.45 (0.0 examples/sec; 474.733 sec/batch)\n",
      "2017-06-21 11:29:05.869149: step 3 batch 210, loss = 10.25 (0.0 examples/sec; 498.312 sec/batch)\n",
      "2017-06-21 11:29:29.478829: step 3 batch 220, loss = 10.01 (0.0 examples/sec; 521.921 sec/batch)\n",
      "2017-06-21 11:29:53.101900: step 3 batch 230, loss = 11.05 (0.0 examples/sec; 545.545 sec/batch)\n",
      "2017-06-21 11:30:16.685410: step 3 batch 240, loss = 11.48 (0.0 examples/sec; 569.128 sec/batch)\n",
      "2017-06-21 11:30:40.376271: step 3 batch 250, loss = 11.82 (0.0 examples/sec; 592.819 sec/batch)\n",
      "2017-06-21 11:31:04.116373: step 3 batch 260, loss = 11.29 (0.0 examples/sec; 616.559 sec/batch)\n",
      "2017-06-21 11:31:27.839626: step 3 batch 270, loss = 15.83 (0.0 examples/sec; 640.282 sec/batch)\n",
      "2017-06-21 11:31:51.419163: step 3 batch 280, loss = 14.11 (0.0 examples/sec; 663.862 sec/batch)\n",
      "2017-06-21 11:32:14.874125: step 3 batch 290, loss = 12.24 (0.0 examples/sec; 687.317 sec/batch)\n",
      "2017-06-21 11:32:38.471633: step 3 batch 300, loss = 9.13 (0.0 examples/sec; 710.914 sec/batch)\n",
      "2017-06-21 11:33:02.036954: step 3 batch 310, loss = 14.90 (0.0 examples/sec; 734.480 sec/batch)\n",
      "2017-06-21 11:33:25.668775: step 3 batch 320, loss = 13.90 (0.0 examples/sec; 758.111 sec/batch)\n",
      "2017-06-21 11:33:49.184670: step 3 batch 330, loss = 10.75 (0.0 examples/sec; 781.627 sec/batch)\n",
      "2017-06-21 11:34:12.696702: step 3 batch 340, loss = 11.49 (0.0 examples/sec; 805.139 sec/batch)\n",
      "2017-06-21 11:34:36.311798: step 3 batch 350, loss = 9.98 (0.0 examples/sec; 828.754 sec/batch)\n",
      "2017-06-21 11:34:59.773478: step 3 batch 360, loss = 17.30 (0.0 examples/sec; 852.216 sec/batch)\n",
      "2017-06-21 11:35:23.483262: step 3 batch 370, loss = 9.34 (0.0 examples/sec; 875.926 sec/batch)\n",
      "2017-06-21 11:35:47.004586: step 3 batch 380, loss = 8.37 (0.0 examples/sec; 899.447 sec/batch)\n",
      "2017-06-21 11:36:10.734660: step 3 batch 390, loss = 11.04 (0.0 examples/sec; 923.177 sec/batch)\n",
      "2017-06-21 11:36:34.332896: step 3 batch 400, loss = 11.10 (0.0 examples/sec; 946.776 sec/batch)\n",
      "2017-06-21 11:36:57.996426: step 3 batch 410, loss = 10.75 (0.0 examples/sec; 970.439 sec/batch)\n",
      "2017-06-21 11:37:21.680714: step 3 batch 420, loss = 11.64 (0.0 examples/sec; 994.123 sec/batch)\n",
      "2017-06-21 11:37:45.200335: step 3 batch 430, loss = 12.72 (0.0 examples/sec; 1017.643 sec/batch)\n",
      "2017-06-21 11:38:08.675024: step 3 batch 440, loss = 11.93 (0.0 examples/sec; 1041.118 sec/batch)\n",
      "2017-06-21 11:38:32.349701: step 3 batch 450, loss = 12.93 (0.0 examples/sec; 1064.792 sec/batch)\n",
      "2017-06-21 11:38:56.080566: step 3 batch 460, loss = 10.35 (0.0 examples/sec; 1088.523 sec/batch)\n",
      "2017-06-21 11:39:19.657608: step 3 batch 470, loss = 10.79 (0.0 examples/sec; 1112.100 sec/batch)\n",
      "2017-06-21 11:39:43.231289: step 3 batch 480, loss = 11.16 (0.0 examples/sec; 1135.674 sec/batch)\n",
      "2017-06-21 11:40:06.741452: step 3 batch 490, loss = 13.20 (0.0 examples/sec; 1159.184 sec/batch)\n",
      "2017-06-21 11:40:30.358521: step 3 batch 500, loss = 10.77 (0.0 examples/sec; 1182.801 sec/batch)\n",
      "2017-06-21 11:40:53.958400: step 3 batch 510, loss = 13.25 (0.0 examples/sec; 1206.401 sec/batch)\n",
      "2017-06-21 11:41:17.521731: step 3 batch 520, loss = 13.42 (0.0 examples/sec; 1229.964 sec/batch)\n",
      "2017-06-21 11:41:41.283363: step 3 batch 530, loss = 9.15 (0.0 examples/sec; 1253.726 sec/batch)\n",
      "2017-06-21 11:42:04.790948: step 3 batch 540, loss = 10.85 (0.0 examples/sec; 1277.234 sec/batch)\n",
      "2017-06-21 11:42:28.330973: step 3 batch 550, loss = 11.39 (0.0 examples/sec; 1300.774 sec/batch)\n",
      "2017-06-21 11:42:51.859053: step 3 batch 560, loss = 10.85 (0.0 examples/sec; 1324.302 sec/batch)\n",
      "2017-06-21 11:43:15.343814: step 3 batch 570, loss = 8.18 (0.0 examples/sec; 1347.786 sec/batch)\n",
      "2017-06-21 11:43:39.044296: step 3 batch 580, loss = 13.22 (0.0 examples/sec; 1371.487 sec/batch)\n",
      "2017-06-21 11:44:02.628806: step 3 batch 590, loss = 8.07 (0.0 examples/sec; 1395.071 sec/batch)\n",
      "2017-06-21 11:44:26.068776: step 3 batch 600, loss = 9.86 (0.0 examples/sec; 1418.511 sec/batch)\n",
      "2017-06-21 11:44:49.615637: step 3 batch 610, loss = 10.75 (0.0 examples/sec; 1442.058 sec/batch)\n",
      "2017-06-21 11:45:03.786128: step 4 batch 0, loss = 11.92 (6.8 examples/sec; 2.347 sec/batch)\n",
      "2017-06-21 11:45:27.362657: step 4 batch 10, loss = 10.89 (0.6 examples/sec; 25.924 sec/batch)\n",
      "2017-06-21 11:45:50.944118: step 4 batch 20, loss = 10.85 (0.3 examples/sec; 49.505 sec/batch)\n",
      "2017-06-21 11:46:14.747043: step 4 batch 30, loss = 8.00 (0.2 examples/sec; 73.308 sec/batch)\n",
      "2017-06-21 11:46:38.553447: step 4 batch 40, loss = 12.38 (0.2 examples/sec; 97.114 sec/batch)\n",
      "2017-06-21 11:47:02.124698: step 4 batch 50, loss = 12.84 (0.1 examples/sec; 120.686 sec/batch)\n",
      "2017-06-21 11:47:25.913330: step 4 batch 60, loss = 12.66 (0.1 examples/sec; 144.474 sec/batch)\n",
      "2017-06-21 11:47:49.427777: step 4 batch 70, loss = 9.19 (0.1 examples/sec; 167.989 sec/batch)\n",
      "2017-06-21 11:48:13.167386: step 4 batch 80, loss = 9.46 (0.1 examples/sec; 191.728 sec/batch)\n",
      "2017-06-21 11:48:36.740545: step 4 batch 90, loss = 13.47 (0.1 examples/sec; 215.302 sec/batch)\n",
      "2017-06-21 11:49:00.285743: step 4 batch 100, loss = 11.30 (0.1 examples/sec; 238.847 sec/batch)\n",
      "2017-06-21 11:49:24.018236: step 4 batch 110, loss = 9.78 (0.1 examples/sec; 262.579 sec/batch)\n",
      "2017-06-21 11:49:47.708122: step 4 batch 120, loss = 9.63 (0.1 examples/sec; 286.269 sec/batch)\n",
      "2017-06-21 11:50:11.251548: step 4 batch 130, loss = 10.70 (0.1 examples/sec; 309.813 sec/batch)\n",
      "2017-06-21 11:50:34.731458: step 4 batch 140, loss = 10.72 (0.0 examples/sec; 333.292 sec/batch)\n",
      "2017-06-21 11:50:58.213511: step 4 batch 150, loss = 9.31 (0.0 examples/sec; 356.775 sec/batch)\n",
      "2017-06-21 11:51:21.791305: step 4 batch 160, loss = 10.77 (0.0 examples/sec; 380.352 sec/batch)\n",
      "2017-06-21 11:51:45.418485: step 4 batch 170, loss = 11.11 (0.0 examples/sec; 403.980 sec/batch)\n",
      "2017-06-21 11:52:08.981509: step 4 batch 180, loss = 11.36 (0.0 examples/sec; 427.543 sec/batch)\n",
      "2017-06-21 11:52:32.569417: step 4 batch 190, loss = 10.85 (0.0 examples/sec; 451.130 sec/batch)\n",
      "2017-06-21 11:52:56.080509: step 4 batch 200, loss = 11.60 (0.0 examples/sec; 474.642 sec/batch)\n",
      "2017-06-21 11:53:19.628278: step 4 batch 210, loss = 12.05 (0.0 examples/sec; 498.189 sec/batch)\n",
      "2017-06-21 11:53:43.193482: step 4 batch 220, loss = 8.73 (0.0 examples/sec; 521.755 sec/batch)\n",
      "2017-06-21 11:54:06.793782: step 4 batch 230, loss = 9.06 (0.0 examples/sec; 545.355 sec/batch)\n",
      "2017-06-21 11:54:30.409619: step 4 batch 240, loss = 9.60 (0.0 examples/sec; 568.971 sec/batch)\n",
      "2017-06-21 11:54:54.010509: step 4 batch 250, loss = 19.30 (0.0 examples/sec; 592.572 sec/batch)\n",
      "2017-06-21 11:55:17.518246: step 4 batch 260, loss = 12.68 (0.0 examples/sec; 616.079 sec/batch)\n",
      "2017-06-21 11:55:41.154997: step 4 batch 270, loss = 8.50 (0.0 examples/sec; 639.716 sec/batch)\n",
      "2017-06-21 11:56:04.769126: step 4 batch 280, loss = 14.89 (0.0 examples/sec; 663.330 sec/batch)\n",
      "2017-06-21 11:56:28.330581: step 4 batch 290, loss = 14.83 (0.0 examples/sec; 686.892 sec/batch)\n",
      "2017-06-21 11:56:52.106621: step 4 batch 300, loss = 14.73 (0.0 examples/sec; 710.668 sec/batch)\n",
      "2017-06-21 11:57:15.762881: step 4 batch 310, loss = 12.74 (0.0 examples/sec; 734.324 sec/batch)\n",
      "2017-06-21 11:57:39.294033: step 4 batch 320, loss = 8.61 (0.0 examples/sec; 757.855 sec/batch)\n",
      "2017-06-21 11:58:02.976469: step 4 batch 330, loss = 14.52 (0.0 examples/sec; 781.538 sec/batch)\n",
      "2017-06-21 11:58:26.740637: step 4 batch 340, loss = 10.00 (0.0 examples/sec; 805.302 sec/batch)\n",
      "2017-06-21 11:58:50.309374: step 4 batch 350, loss = 12.26 (0.0 examples/sec; 828.870 sec/batch)\n",
      "2017-06-21 11:59:14.159595: step 4 batch 360, loss = 14.09 (0.0 examples/sec; 852.721 sec/batch)\n",
      "2017-06-21 11:59:37.852826: step 4 batch 370, loss = 7.97 (0.0 examples/sec; 876.414 sec/batch)\n",
      "2017-06-21 12:00:01.330824: step 4 batch 380, loss = 13.85 (0.0 examples/sec; 899.892 sec/batch)\n",
      "2017-06-21 12:00:24.734410: step 4 batch 390, loss = 7.62 (0.0 examples/sec; 923.295 sec/batch)\n",
      "2017-06-21 12:00:48.189394: step 4 batch 400, loss = 10.42 (0.0 examples/sec; 946.750 sec/batch)\n",
      "2017-06-21 12:01:11.851698: step 4 batch 410, loss = 10.70 (0.0 examples/sec; 970.413 sec/batch)\n",
      "2017-06-21 12:01:35.429204: step 4 batch 420, loss = 11.34 (0.0 examples/sec; 993.990 sec/batch)\n",
      "2017-06-21 12:01:59.077281: step 4 batch 430, loss = 11.95 (0.0 examples/sec; 1017.638 sec/batch)\n",
      "2017-06-21 12:02:22.640273: step 4 batch 440, loss = 11.98 (0.0 examples/sec; 1041.201 sec/batch)\n",
      "2017-06-21 12:02:46.538428: step 4 batch 450, loss = 12.14 (0.0 examples/sec; 1065.099 sec/batch)\n",
      "2017-06-21 12:03:10.513559: step 4 batch 460, loss = 8.57 (0.0 examples/sec; 1089.075 sec/batch)\n",
      "2017-06-21 12:03:34.204247: step 4 batch 470, loss = 14.52 (0.0 examples/sec; 1112.765 sec/batch)\n",
      "2017-06-21 12:03:57.948682: step 4 batch 480, loss = 10.67 (0.0 examples/sec; 1136.510 sec/batch)\n",
      "2017-06-21 12:04:21.643416: step 4 batch 490, loss = 15.38 (0.0 examples/sec; 1160.204 sec/batch)\n",
      "2017-06-21 12:04:45.419062: step 4 batch 500, loss = 11.63 (0.0 examples/sec; 1183.980 sec/batch)\n",
      "2017-06-21 12:05:08.935059: step 4 batch 510, loss = 12.12 (0.0 examples/sec; 1207.496 sec/batch)\n",
      "2017-06-21 12:05:32.309122: step 4 batch 520, loss = 8.88 (0.0 examples/sec; 1230.870 sec/batch)\n",
      "2017-06-21 12:05:55.929800: step 4 batch 530, loss = 10.64 (0.0 examples/sec; 1254.491 sec/batch)\n",
      "2017-06-21 12:06:19.508398: step 4 batch 540, loss = 11.49 (0.0 examples/sec; 1278.069 sec/batch)\n",
      "2017-06-21 12:06:43.049070: step 4 batch 550, loss = 14.44 (0.0 examples/sec; 1301.610 sec/batch)\n",
      "2017-06-21 12:07:06.719472: step 4 batch 560, loss = 14.19 (0.0 examples/sec; 1325.281 sec/batch)\n",
      "2017-06-21 12:07:30.421305: step 4 batch 570, loss = 12.26 (0.0 examples/sec; 1348.982 sec/batch)\n",
      "2017-06-21 12:07:53.960480: step 4 batch 580, loss = 10.97 (0.0 examples/sec; 1372.522 sec/batch)\n",
      "2017-06-21 12:08:17.560363: step 4 batch 590, loss = 6.66 (0.0 examples/sec; 1396.121 sec/batch)\n",
      "2017-06-21 12:08:41.226421: step 4 batch 600, loss = 12.57 (0.0 examples/sec; 1419.787 sec/batch)\n",
      "2017-06-21 12:09:04.732713: step 4 batch 610, loss = 13.12 (0.0 examples/sec; 1443.294 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "for step in range(5):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    random.shuffle(record_list)\n",
    "    for i_batch in range(num_batch_per_epoch):\n",
    "        if i_batch + 1 == num_batch_per_epoch:\n",
    "            break\n",
    "#             l = record_list[i_batch * batch_size:]\n",
    "        else:\n",
    "            l = record_list[i_batch * batch_size:i_batch * batch_size + batch_size]\n",
    "        np_images = []\n",
    "        np_labels = []\n",
    "        np_objects_num = []\n",
    "        for item in l:\n",
    "            image, label, object_num = record_process(item)\n",
    "            image = np.array(image)\n",
    "            image = image.astype(np.float32)\n",
    "            np_images.append(image)\n",
    "            np_labels.append(label)\n",
    "            np_objects_num.append(object_num)\n",
    "        np_images = np.asarray(np_images, dtype=np.float32)\n",
    "        np_images = np_images/255 * 2 - 1\n",
    "        np_labels = np.asarray(np_labels, dtype=np.float32)\n",
    "        np_objects_num = np.asarray(np_objects_num, dtype=np.int32)\n",
    "        _, loss_value = sess.run([opt, total_loss], feed_dict={images: np_images, labels: np_labels, objects_num: np_objects_num})\n",
    "        duration = time.time() - start_time\n",
    "        if i_batch % 10 == 0:\n",
    "            num_examples_per_step = batch_size\n",
    "            examples_per_sec = num_examples_per_step / duration\n",
    "            sec_per_batch = float(duration)\n",
    "\n",
    "            format_str = ('%s: step %d batch %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                          'sec/batch)')\n",
    "            print (format_str % (datetime.now(), step, i_batch, loss_value,\n",
    "                                 examples_per_sec, sec_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver()\n",
    "saveDir = 'yolo2d_model'\n",
    "saver.save(sess, saveDir + '/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58.176834106445312, 77.929512023925781, 389.38887405395508, 369.74724578857422, 'cat')\n"
     ]
    }
   ],
   "source": [
    "##test\n",
    "def process_predicts(predicts):\n",
    "    p_classes = predicts[0, :, :, 0:20]\n",
    "    C = predicts[0, :, :, 20:22]\n",
    "    coordinate = predicts[0, :, :, 22:]\n",
    "\n",
    "    p_classes = np.reshape(p_classes, (7, 7, 1, 20))\n",
    "    C = np.reshape(C, (7, 7, 2, 1))\n",
    "\n",
    "    P = C * p_classes\n",
    "\n",
    "    #print P[5,1, 0, :]\n",
    "\n",
    "    index = np.argmax(P)\n",
    "\n",
    "    index = np.unravel_index(index, P.shape)\n",
    "\n",
    "    class_num = index[3]\n",
    "\n",
    "    coordinate = np.reshape(coordinate, (7, 7, 2, 4))\n",
    "\n",
    "    max_coordinate = coordinate[index[0], index[1], index[2], :]\n",
    "\n",
    "    xcenter = max_coordinate[0]\n",
    "    ycenter = max_coordinate[1]\n",
    "    w = max_coordinate[2]\n",
    "    h = max_coordinate[3]\n",
    "\n",
    "    xcenter = (index[1] + xcenter) * (448/7.0)\n",
    "    ycenter = (index[0] + ycenter) * (448/7.0)\n",
    "\n",
    "    w = w * 448\n",
    "    h = h * 448\n",
    "\n",
    "    xmin = xcenter - w/2.0\n",
    "    ymin = ycenter - h/2.0\n",
    "\n",
    "    xmax = xmin + w\n",
    "    ymax = ymin + h\n",
    "\n",
    "    return xmin, ymin, xmax, ymax, class_num\n",
    "\n",
    "classes_name =  [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\",\"tvmonitor\"]\n",
    "\n",
    "np_images = []\n",
    "# np_img = cv2.imread('cat.jpg')\n",
    "np_img = Image.open('cat.jpg')\n",
    "# resized_img = cv2.resize(np_img, (height, width))\n",
    "resized_img = np_img.resize((height, width))\n",
    "# np_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
    "np_img = cvtColor(resized_img)\n",
    "np_img = np.array(np_img)\n",
    "np_img = np_img.astype(np.float32)\n",
    "np_images.append(np_img)\n",
    "np_images = np.asarray(np_images, dtype=np.float32)\n",
    "np_images = np_images/255 * 2 - 1\n",
    "np_predict = sess.run(predicts, feed_dict={images: np_images})\n",
    "\n",
    "xmin, ymin, xmax, ymax, class_num = process_predicts(np_predict)\n",
    "class_name = classes_name[class_num]\n",
    "print(xmin, ymin, xmax, ymax, class_name)\n",
    "# cv2.rectangle(resized_img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 0, 255))\n",
    "# cv2.putText(resized_img, class_name, (int(xmin), int(ymin)), 2, 1.5, (0, 0, 255))\n",
    "# cv2.imwrite('cat_out.jpg', resized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
